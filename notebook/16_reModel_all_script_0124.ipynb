{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"16_reModel_all_script_0124.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vdjooqENT44U","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E44XcSxVUAW-","colab_type":"code","colab":{}},"source":["!pip install transformers\n","!pip install flashtext"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wm3zsVcyUNAC","colab_type":"code","colab":{}},"source":["\n","import pandas as pd\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import os, sys, gc, random, multiprocessing, glob, time\n","\n","DATA_DIR = '/content/drive/My Drive/Colab Notebooks/GoogleQuest/input/google-quest-challenge'\n","# DATA_DIR = '../input/google-quest-challenge'\n","# DATA_DIR = 'D:/project/ICF_AutoCapsule_disabled/kaggle/google-quest-challenge'\n","# BERT_DIR = 'D:/project/ICF_AutoCapsule_disabled/BERT'\n","\n","\n","# In[ ]:\n","\n","\n","# !pip install ../input/sacremoses/sacremoses-master/\n","# !pip install ../input/transformers/transformers-master/\n","\n","\n","# In[4]:\n","\n","\n","# !pip install transformers\n","# !pip install flashtext\n","\n","\n","# In[5]:\n","\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils import data\n","from torch.utils.data import DataLoader, Dataset\n","\n","#from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n","from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import LabelEncoder\n","\n","from scipy.stats import spearmanr\n","\n","import transformers\n","from transformers import (\n","    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n","    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n","    get_cosine_schedule_with_warmup,\n",")\n","\n","from tqdm import tqdm\n","print(transformers.__version__)\n","\n","\n","# In[ ]:\n","\n","\n","## Make results reproducible .Else noone will believe you .\n","import random\n","\n","def seed_everything(seed: int):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","# In[ ]:\n","\n","\n","class PipeLineConfig:\n","    def __init__(self, lr, warmup, epochs, patience, batch_size, seed, name, question_weight,answer_weight,fold,train):\n","        self.lr = lr\n","        self.warmup = warmup\n","        self.epochs = epochs\n","        self.patience = patience\n","        self.batch_size = batch_size\n","        self.seed = seed\n","        self.name = name\n","        self.question_weight = question_weight\n","        self.answer_weight =answer_weight\n","        self.fold = fold\n","        self.train = train\n","\n","\n","# In[ ]:\n","\n","\n","config = PipeLineConfig(lr=1e-5,                         warmup=0.01,                         epochs=4,                         patience=3,                         batch_size=8,                         seed=42,                         name='reModel_AB10',                         question_weight=0.5,                         answer_weight=0.5,                         fold=5,                         train=True\n","                       )\n","\n","\n","# In[ ]:\n","\n","\n","seed_everything(config.seed)\n","\n","\n","# In[10]:\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#device = 'cpu'\n","print(device)\n","\n","\n","# In[11]:\n","\n","\n","sub = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n","sub.head()\n","\n","\n","# In[12]:\n","\n","\n","target_columns = sub.columns.values[1:].tolist()\n","target_columns\n","\n","\n","# In[ ]:\n","\n","\n","target_columns = [\n","        'question_asker_intent_understanding',\n","        'question_body_critical',\n","        'question_well_written',\n","        'question_conversational',\n","        'question_opinion_seeking',\n","        'question_not_really_a_question'\n","]\n","\n","\n","# In[14]:\n","\n","\n","train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","train.head()\n","\n","\n","# In[15]:\n","\n","\n","test = pd.read_csv(f'{DATA_DIR}/test.csv')\n","test.head()\n","\n","\n","# ## Preprocessing\n","\n","# In[ ]:\n","\n","\n","import re\n","from flashtext import KeywordProcessor\n","\n","\n","# In[ ]:\n","\n","\n","PUNCTS = {\n","            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n","            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n","            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n","            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n","            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n","            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n","            '↑', 'º', '¯', '♫', '#'\n","          }\n","\n","\n","mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n","\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n","\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n","\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n","\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n","\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n","\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n","\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n","\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n","\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n","\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n","\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n","\n","\n","kp = KeywordProcessor(case_sensitive=True)\n","for k, v in mispell_dict.items():\n","    kp.add_keyword(k, v)\n","\n","def clean_punct(text):\n","    text = str(text)\n","    for punct in PUNCTS:\n","        text = text.replace(punct, ' {} '.format(punct))\n","    return text\n","\n","\n","def preprocessing(text):\n","    text = text.lower()\n","    text = re.sub(r'(\\&lt)|(\\&gt)', ' ', text)\n","    \n","    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n","    text = kp.replace_keywords(text)\n","    text = clean_punct(text)\n","    text = re.sub(r'\\n\\r', ' ', text)\n","    text = re.sub(r'\\s{2,}', ' ', text)\n","    \n","    return text\n","\n","\n","# In[ ]:\n","\n","\n","category_list = ['CULTURE', 'LIFE_ARTS', 'SCIENCE', 'STACKOVERFLOW', 'TECHNOLOGY']\n","\n","host_list = ['academia.stackexchange.com', 'android.stackexchange.com',\n","       'anime.stackexchange.com', 'apple.stackexchange.com',\n","       'askubuntu.com', 'bicycles.stackexchange.com',\n","       'biology.stackexchange.com', 'blender.stackexchange.com',\n","       'boardgames.stackexchange.com', 'chemistry.stackexchange.com',\n","       'christianity.stackexchange.com', 'codereview.stackexchange.com',\n","       'cooking.stackexchange.com', 'crypto.stackexchange.com',\n","       'cs.stackexchange.com', 'dba.stackexchange.com',\n","       'diy.stackexchange.com', 'drupal.stackexchange.com',\n","       'dsp.stackexchange.com', 'electronics.stackexchange.com',\n","       'ell.stackexchange.com', 'english.stackexchange.com',\n","       'expressionengine.stackexchange.com', 'gamedev.stackexchange.com',\n","       'gaming.stackexchange.com', 'gis.stackexchange.com',\n","       'graphicdesign.stackexchange.com', 'judaism.stackexchange.com',\n","       'magento.stackexchange.com', 'math.stackexchange.com',\n","       'mathematica.stackexchange.com', 'mathoverflow.net',\n","       'mechanics.stackexchange.com', 'meta.askubuntu.com',\n","       'meta.christianity.stackexchange.com',\n","       'meta.codereview.stackexchange.com', 'meta.math.stackexchange.com',\n","       'meta.stackexchange.com', 'meta.superuser.com',\n","       'money.stackexchange.com', 'movies.stackexchange.com',\n","       'music.stackexchange.com', 'photo.stackexchange.com',\n","       'physics.stackexchange.com', 'programmers.stackexchange.com',\n","       'raspberrypi.stackexchange.com', 'robotics.stackexchange.com',\n","       'rpg.stackexchange.com', 'salesforce.stackexchange.com',\n","       'scifi.stackexchange.com', 'security.stackexchange.com',\n","       'serverfault.com', 'sharepoint.stackexchange.com',\n","       'softwarerecs.stackexchange.com', 'stackoverflow.com',\n","       'stats.stackexchange.com', 'superuser.com',\n","       'tex.stackexchange.com', 'travel.stackexchange.com',\n","       'unix.stackexchange.com', 'ux.stackexchange.com',\n","       'webapps.stackexchange.com', 'webmasters.stackexchange.com',\n","       'wordpress.stackexchange.com']\n","\n","\n","# ## Dataset\n","\n","# In[ ]:\n","\n","\n","MAX_LEN = 512\n","MAX_T_LEN = 30\n","MAX_Q_LEN = 512-30-3\n","SEP_TOKEN_ID = 102 # bert-base-uncasedにおけるvocabの'[SEP']が、102番目という意味\n","\n","class QuestDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, train_mode=True, labeled=True):\n","        self.df = df\n","        self.train_mode = train_mode\n","        self.labeled = labeled\n","        #self.tokenizer = BertTokenizer.from_pretrained(BERT_DIR+'/bert-base-uncased')\n","        #self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased/')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        token_id列\n","        segment_id列\n","        label列\n","        \"\"\"\n","        row = self.df.iloc[index]\n","        token_ids, seg_ids = self.get_token_ids(row)\n","        if self.labeled:\n","            labels = self.get_label(row)\n","            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host), labels\n","        else:\n","            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","#     def select_tokens(self, tokens, max_num):\n","#         if len(tokens) <= max_num:\n","#             return tokens\n","#         if self.train_mode:\n","#             num_remove = len(tokens) - max_num\n","#             remove_start = random.randint(0, len(tokens)-num_remove-1)\n","#             return tokens[:remove_start] + tokens[remove_start + num_remove:]\n","#         else:\n","#             return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n","\n","    def trim_input(self, title, question, max_sequence_length=MAX_LEN, \n","                t_max_len=MAX_T_LEN, q_max_len=MAX_Q_LEN):\n","        \"\"\"\n","        title. question, answerそれぞれのセンテンスを、tokenizeする\n","        max_lengthに足りない分は、\n","        \"\"\"\n","        t = self.tokenizer.tokenize(title)\n","        q = self.tokenizer.tokenize(question)\n","\n","\n","        t = t[:t_max_len]\n","        q = q[:q_max_len]\n","\n","        if len(t) < MAX_T_LEN:\n","            \"\"\"0で後ろからpadding\"\"\"\n","            t += [0] * (MAX_T_LEN - len(t))\n","\n","        if len(q) < MAX_Q_LEN:\n","            \"\"\"0で後ろからpadding\"\"\"\n","            q += [0] * (MAX_Q_LEN - len(q))\n","\n","        return t, q\n","        \n","    def get_token_ids(self, row):\n","        t_tokens, q_tokens = self.trim_input(row.question_title, row.question_body)\n","        \n","        # BERTの入力タイプに変換([CLS]と[SEP]をつないで、１つのsetentenceに)\n","        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]']\n","        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        ids = torch.tensor(token_ids)\n","        seg_ids = self.get_seg_ids(ids)  # segment_embを区別するindex\n","        return ids, seg_ids\n","    \n","    def get_seg_ids(self, ids):\n","        \"\"\"\n","        いくつめの文かを区別するsegment_idを、各文字に振る\n","        \"\"\"\n","        seg_ids = torch.zeros_like(ids) # [max_len]のtorch_tensor\n","        seg_idx = 0\n","        first_sep = True\n","        for i, e in enumerate(ids):\n","            seg_ids[i] = seg_idx\n","            if e == SEP_TOKEN_ID: # [SEP]の場合\n","                seg_idx = 1\n","        pad_idx = torch.nonzero(ids == 0)  # bert-base_uncasedのvocabで、[PAD]は0番目であるので、PADの部分のindexだけ抽出\n","        seg_ids[pad_idx] = 0\n","\n","        return seg_ids\n","\n","    def get_label(self, row):\n","        #print(row[target_columns].values)\n","        return torch.tensor(row[target_columns].values.astype(np.float32))\n","\n","    def collate_fn(self, batch):\n","        \"\"\"\n","        labelデータを持つモードと、ない完全な推論モードでは、batchのshapeが異なるので(labelが2番目の要素にあるなし)\n","        \"\"\"\n","        token_ids = torch.stack([x[0] for x in batch])\n","        seg_ids = torch.stack([x[1] for x in batch])\n","        category = torch.stack([x[2] for x in batch])\n","        host = torch.stack([x[3] for x in batch])\n","    \n","        if self.labeled:\n","            labels = torch.stack([x[-1] for x in batch])\n","            return token_ids, seg_ids, category, host, labels\n","        else:\n","            return token_ids, seg_ids, category, host\n","\n","\n","# In[ ]:\n","\n","\n","def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n","    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    df = shuffle(df, random_state=1234)\n","    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n","    for fold, (train_idx, valid_idx) in enumerate(gkf):\n","        if fold == ifold:\n","            df_train = df.iloc[train_idx]\n","            df_val = df.iloc[valid_idx]\n","            break\n","\n","    print('train', df_train.shape)\n","    print('val', df_val.shape)\n","\n","    ds_train = QuestDataset(df_train, train_mode=True)\n","    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n","    train_loader.num = len(df_train)\n","\n","    ds_val = QuestDataset(df_val, train_mode=False)\n","    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n","    val_loader.num = len(df_val)\n","    val_loader.df = df_val\n","\n","    return train_loader, val_loader, df_val.shape[0]\n","\n","\n","def get_train_loader(batch_size=4):\n","    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n","    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n","    loader.num = len(df)\n","    \n","    return loader\n","\n","\n","def get_test_loader(batch_size=4):\n","    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n","    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n","    loader.num = len(df)\n","    \n","    return loader\n","\n","\n","# In[ ]:\n","\n","\n","class QuestModel(nn.Module):\n","    def __init__(self, n_classes=30):\n","        super(QuestModel, self).__init__()\n","        self.model_name = 'QuestModel'\n","        #self.bert_model = BertModel.from_pretrained(BERT_DIR+'/bert-base-uncased/')\n","        #self.bert_model = BertModel.from_pretrained('../input/bert-base-uncased/')\n","        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n","        \n","        self.emb_category = nn.Embedding(len(category_list), 3)\n","        self.emb_host = nn.Embedding(len(host_list), 5)\n","        \n","        self.fc = nn.Linear(768+3+5, n_classes)\n","\n","    def forward(self, ids, seg_ids, category, host):\n","        attention_mask = (ids > 0)  # ids==0([PAD])部分だけFalseとなるので、そこだけattention_weightを0に\n","        layers, pool_out = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n","        #print(layers.size())  # (batch_size,sequence_length, 768)\n","        #print(pool_out.size())  # (batch_size, 768), first token of last layerをいじったもの\n","        \n","        out = F.avg_pool1d(layers.transpose(1,2), kernel_size=layers.size()[1]).squeeze()  # sequence方向は中央値だけ抽出\n","        \n","        emb_category = self.emb_category(category)\n","        emb_host = self.emb_host(host)\n","        \n","        #print(out.shape)\n","        #print(emb_category.shape)\n","        \n","        out = torch.cat([out, emb_category], dim=-1)\n","        out = torch.cat([out, emb_host], dim=-1)\n","        \n","        #print(out.shape)\n","        \n","        out = F.dropout(out, p=0.2, training=self.training)\n","        \n","#         out = F.dropout(layers[-1][:, 0, :], p=0.2, training=self.training)\n","#         out =  F.dropout(pool_out, p=0.2, training=self.training)\n","        logit = self.fc(out)\n","        return logit # 単に30種類の出力値を算出\n","    \n","\n","\n","# In[ ]:\n","\n","\n","def train_model(train_loader, optimizer, criterion, scheduler):\n","    model.train()\n","    avg_loss = 0.    \n","    for idx, batch in enumerate(tqdm(train_loader)):\n","        ids_train, seg_ids_train, category_train, host_train, label_ids_train = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n","        \n","        #print(host_train)\n","        \n","        logits = model(ids_train, seg_ids_train, category_train, host_train)\n","        #logits = torch.sigmoid(model(ids_train, seg_ids_train))\n","        \n","        #loss = config.question_weight*criterion(logits[:,0:21], label_ids_train[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], label_ids_train[:,21:30])\n","        loss = criterion(logits, label_ids_train)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        \n","        avg_loss += loss.item() / len(train_loader)\n","        del ids_train, seg_ids_train, label_ids_train\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    return avg_loss\n","\n","def val_model(val_loader, val_length, batch_size=8):\n","    model.eval() # eval mode  \n","    avg_val_loss = 0.\n","    \n","    valid_preds = np.zeros((val_length, len(target_columns)))\n","    original = np.zeros((val_length, len(target_columns)))\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(tqdm(val_loader)):\n","            ids_val, seg_ids_val, category_val, host_val, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n","            \n","            logits = torch.sigmoid(model(ids_val, seg_ids_val, category_val, host_val))\n","            \n","            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n","            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n","            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n","        \n","        score = 0\n","        preds = torch.tensor(valid_preds).numpy()\n","        #preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n","        \n","        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n","        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n","        \n","        for i in range(len(target_columns)):\n","            print(i, spearmanr(original[:,i], preds[:,i]))\n","            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n","    \n","    return avg_val_loss, score/len(target_columns)\n","\n","\n","# In[ ]:\n","\n","\n","def calc_spearman(targets, preds):\n","    score = 0\n","    for i in range(targets.shape[1]):\n","        score += np.nan_to_num(spearmanr(targets[:, i], preds[:, i]).correlation)\n","    return score/targets.shape[1]\n","\n","\n","# In[ ]:\n","\n","\n","ACCUM_STEPS = 1\n","\n","\n","# In[ ]:\n","\n","\n","# model = QuestModel(n_classes=len(target_columns)).to(device)\n","# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n","# criterion = nn.BCEWithLogitsLoss()\n","\n","\n","# # In[ ]:\n","\n","\n","# print(config.name)\n","# for fold in range(config.fold):\n","#     print('---%d-Fold---'%(fold+1))\n","    \n","#     patience = 0\n","#     best_loss   = 100.0\n","#     best_score      = -1.\n","#     best_preds = 0\n","#     best_param_loss = None\n","#     best_param_score = None\n","    \n","#     for epoch in range(config.epochs):\n","        \n","#         torch.cuda.empty_cache()\n","#         start_time   = time.time()\n","        \n","#         train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n","#         scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n","        \n","#         loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n","#         loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n","#         print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n","        \n","\n","#         if score_val > best_score:\n","#             best_score = score_val\n","#             best_param_score = model.state_dict()\n","#             print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","#             torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","#         else:\n","#             patience += 1\n","#             if patience >= config.patience:\n","#                 del train_loader, val_loader, loss_train, loss_val, score_val\n","#                 torch.cuda.empty_cache()\n","#                 gc.collect()\n","#                 break\n","    \n","#         del train_loader, val_loader, loss_train, loss_val, score_val\n","#         torch.cuda.empty_cache()\n","#         gc.collect()\n","        \n","#     model.load_state_dict(best_param_score)\n","#     print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","#     torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n","\n","#     torch.cuda.empty_cache()\n","#     gc.collect()\n","\n","\n","# # In[ ]:\n","\n","\n","# def create_model(model_file):\n","#     model = QuestModel(n_classes=len(target_columns)).to(device)\n","#     model.load_state_dict(torch.load(model_file))\n","#     model = model\n","#     #model = DataParallel(model)\n","#     return model\n","\n","# def create_models():\n","#     models = []\n","#     for fold in range(config.fold):\n","#         model = create_model('/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","#         model.eval()\n","#         models.append(model)\n","#     return models\n","\n","\n","# def predict(models, test_loader):\n","#     all_scores = []\n","#     with torch.no_grad():\n","#         for ids, seg_ids, category, host in tqdm(test_loader, total=test_loader.num // test_loader.batch_size):\n","#             ids, seg_ids, category, host = ids.to(device), seg_ids.to(device), category.to(device), host.to(device)\n","#             scores = []\n","#             for model in models:\n","#                 outputs = torch.sigmoid(model(ids, seg_ids, category, host)).cpu()\n","#                 scores.append(outputs)\n","#             all_scores.append(torch.mean(torch.stack(scores), 0))\n","\n","#     all_scores = torch.cat(all_scores, 0).numpy()\n","    \n","#     return all_scores\n","\n","\n","# # In[ ]:\n","\n","\n","# # train_loader = get_train_loader(batch_size=32)\n","# # models = create_models()\n","# # preds = predict(models, train_loader)\n","# # train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","# # cv_score = calc_spearman(train[target_columns].values, preds)\n","# # print(cv_score)\n","\n","# # del train_loader, models, preds, train\n","# # torch.cuda.empty_cache()\n","# # gc.collect()\n","\n","\n","# # In[ ]:\n","\n","\n","# with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}.txt'.format(config.name), 'w') as f:\n","#     f.write('')\n","\n","\n","# ## AI\n","\n","# In[ ]:\n","\n","\n","config = PipeLineConfig(lr=1e-5,                         warmup=0.01,                         epochs=4,                         patience=3,                         batch_size=8,                         seed=42,                         name='reModel_AI',                         question_weight=0.5,                         answer_weight=0.5,                         fold=5,                         train=True\n","                       )\n","\n","\n","# In[ ]:\n","\n","\n","target_columns = [\n","    'question_asker_intent_understanding',\n","    'question_body_critical',\n","    'question_well_written',\n","    'question_type_compare',\n","    'question_type_consequence',\n","    'question_type_definition',\n","    'question_type_entity'\n","]\n","\n","\n","# In[ ]:\n","model = QuestModel(n_classes=len(target_columns)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","\n","print(config.name)\n","for fold in range(config.fold):\n","    print('---%d-Fold---'%(fold+1))\n","    \n","    patience = 0\n","    best_loss   = 100.0\n","    best_score      = -1.\n","    best_preds = 0\n","    best_param_loss = None\n","    best_param_score = None\n","    \n","    for epoch in range(config.epochs):\n","        \n","        torch.cuda.empty_cache()\n","        start_time   = time.time()\n","        \n","        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n","        \n","        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n","        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n","        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n","        \n","\n","        if score_val > best_score:\n","            best_score = score_val\n","            best_param_score = model.state_dict()\n","            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","        else:\n","            patience += 1\n","            if patience >= config.patience:\n","                del train_loader, val_loader, loss_train, loss_val, score_val\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                break\n","    \n","        del train_loader, val_loader, loss_train, loss_val, score_val\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    model.load_state_dict(best_param_score)\n","    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","# train_loader = get_train_loader(batch_size=32)\n","# models = create_models()\n","# preds = predict(models, train_loader)\n","# train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","# cv_score = calc_spearman(train[target_columns].values, preds)\n","# print(cv_score)\n","\n","# del train_loader, models, preds, train\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}.txt'.format(config.name), 'w') as f:\n","    f.write('')\n","\n","\n","# ## Question + Answer\n","\n","# In[ ]:\n","\n","\n","MAX_LEN = 512\n","#MAX_Q_LEN = 250\n","#MAX_A_LEN = 259\n","SEP_TOKEN_ID = 102 # bert-base-uncasedにおけるvocabの'[SEP']が、102番目という意味\n","\n","class QuestDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, train_mode=True, labeled=True):\n","        self.df = df\n","        self.train_mode = train_mode\n","        self.labeled = labeled\n","        #self.tokenizer = BertTokenizer.from_pretrained(BERT_DIR+'/bert-base-uncased')\n","        #self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased/')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        token_id列\n","        segment_id列\n","        label列\n","        \"\"\"\n","        row = self.df.iloc[index]\n","        token_ids, seg_ids = self.get_token_ids(row)\n","        if self.labeled:\n","            labels = self.get_label(row)\n","            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host), labels\n","        else:\n","            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","\n","#     def select_tokens(self, tokens, max_num):\n","#         if len(tokens) <= max_num:\n","#             return tokens\n","#         if self.train_mode:\n","#             num_remove = len(tokens) - max_num\n","#             remove_start = random.randint(0, len(tokens)-num_remove-1)\n","#             return tokens[:remove_start] + tokens[remove_start + num_remove:]\n","#         else:\n","#             return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n","\n","    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n","                t_max_len=30, q_max_len=239, a_max_len=239):\n","        \"\"\"\n","        title. question, answerそれぞれのセンテンスを、tokenizeする\n","        max_lengthに足りない分は、\n","        \"\"\"\n","        t = self.tokenizer.tokenize(title)\n","        q = self.tokenizer.tokenize(question)\n","        a = self.tokenizer.tokenize(answer)\n","\n","        t_len = len(t)\n","        q_len = len(q)\n","        a_len = len(a)\n","\n","        if (t_len+q_len+a_len+4) > max_sequence_length:\n","\n","            if t_max_len > t_len:\n","                \"\"\"\n","                titleが短い場合、\n","                最大長に足りない長さを、半分ずつqとaに加える\n","                \"\"\"\n","                t_new_len = t_len\n","                a_max_len = a_max_len + math.floor((t_max_len - t_len)/2) # 切り捨て\n","                q_max_len = q_max_len + math.ceil((t_max_len - t_len)/2) # 切り上げ\n","            else:\n","                \"\"\"\n","                titleが長い場合、最大長で切る\n","                \"\"\"\n","                t_new_len = t_max_len\n","\n","            if a_max_len > a_len:\n","                \"\"\"\n","                answerに加えても短い場合、\n","                最大長に足りない長さを、qに加える\n","                \"\"\"\n","                a_new_len = a_len \n","                q_new_len = q_max_len + (a_max_len - a_len)\n","            elif q_max_len > q_len:\n","                a_new_len = a_max_len + (q_max_len - q_len)\n","                q_new_len = q_len\n","            else:\n","                a_new_len = a_max_len\n","                q_new_len = q_max_len\n","\n","\n","            if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n","                raise ValueError(\"New sequence length should be %d, but is %d\" \n","                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n","\n","            t = t[:t_new_len]\n","            q = q[:q_new_len]\n","            a = a[:a_new_len]\n","\n","        return t, q, a\n","        \n","    def get_token_ids(self, row):\n","        t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer)\n","        \n","        # BERTの入力タイプに変換([CLS]と[SEP]をつないで、１つのsetentenceに)\n","        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n","        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","        if len(token_ids) < MAX_LEN:\n","            \"\"\"0で後ろからpadding\"\"\"\n","            token_ids += [0] * (MAX_LEN - len(token_ids))\n","        ids = torch.tensor(token_ids)\n","        seg_ids = self.get_seg_ids(ids)  # segment_embを区別するindex\n","        return ids, seg_ids\n","    \n","    def get_seg_ids(self, ids):\n","        \"\"\"\n","        いくつめの文かを区別するsegment_idを、各文字に振る\n","        \"\"\"\n","        seg_ids = torch.zeros_like(ids) # [max_len]のtorch_tensor\n","        seg_idx = 0\n","        first_sep = True\n","        for i, e in enumerate(ids):\n","            seg_ids[i] = seg_idx\n","            if e == SEP_TOKEN_ID: # [SEP]の場合\n","                if first_sep:\n","                    first_sep = False\n","                else:\n","                    seg_idx = 1\n","        pad_idx = torch.nonzero(ids == 0)  # bert-base_uncasedのvocabで、[PAD]は0番目であるので、PADの部分のindexだけ抽出\n","        seg_ids[pad_idx] = 0\n","\n","        return seg_ids\n","\n","    def get_label(self, row):\n","        #print(row[target_columns].values)\n","        return torch.tensor(row[target_columns].values.astype(np.float32))\n","\n","    def collate_fn(self, batch):\n","        \"\"\"\n","        labelデータを持つモードと、ない完全な推論モードでは、batchのshapeが異なるので(labelが2番目の要素にあるなし)\n","        \"\"\"\n","        token_ids = torch.stack([x[0] for x in batch])\n","        seg_ids = torch.stack([x[1] for x in batch])\n","        category = torch.stack([x[2] for x in batch])\n","        host = torch.stack([x[3] for x in batch])\n","    \n","        if self.labeled:\n","            labels = torch.stack([x[-1] for x in batch])\n","            return token_ids, seg_ids, category, host, labels\n","        else:\n","            return token_ids, seg_ids, category, host\n","\n","\n","# In[ ]:\n","\n","\n","def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n","    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    df = shuffle(df, random_state=1234)\n","    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n","    for fold, (train_idx, valid_idx) in enumerate(gkf):\n","        if fold == ifold:\n","            df_train = df.iloc[train_idx]\n","            df_val = df.iloc[valid_idx]\n","            break\n","\n","    print('train', df_train.shape)\n","    print('val', df_val.shape)\n","\n","    ds_train = QuestDataset(df_train, train_mode=True)\n","    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n","    train_loader.num = len(df_train)\n","\n","    ds_val = QuestDataset(df_val, train_mode=False)\n","    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n","    val_loader.num = len(df_val)\n","    val_loader.df = df_val\n","\n","    return train_loader, val_loader, df_val.shape[0]\n","\n","\n","def get_train_loader(batch_size=4):\n","    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n","    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n","    loader.num = len(df)\n","    \n","    return loader\n","\n","\n","def get_test_loader(batch_size=4):\n","    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n","\n","    # cleaning\n","    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n","    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n","    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n","    \n","    # label encode\n","    le_category = LabelEncoder()\n","    le_category.fit(category_list)\n","    for c in set(df.category):\n","        if c not in category_list:\n","            df.category = df.category.replace(c, np.nan)\n","            df.category = df.category.fillna(train.category.mode()[0])\n","    df.category = le_category.transform(df.category)\n","    \n","    le_host = LabelEncoder()\n","    le_host.fit(host_list)\n","    for c in set(df.host):\n","        if c not in host_list:\n","            df.host = df.host.replace(c, np.nan)\n","            df.host = df.host.fillna(train.host.mode()[0])\n","    df.host = le_host.transform(df.host)\n","\n","\n","    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n","    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n","    loader.num = len(df)\n","    \n","    return loader\n","\n","\n","# ## AE\n","\n","# In[ ]:\n","\n","\n","config = PipeLineConfig(lr=1e-5,                         warmup=0.01,                         epochs=4,                         patience=3,                         batch_size=8,                         seed=42,                         name='reModel_AE',                         question_weight=0.5,                         answer_weight=0.5,                         fold=5,                         train=True\n","                       )\n","\n","\n","# In[ ]:\n","\n","\n","target_columns = [\n","    'question_asker_intent_understanding',\n","    'question_body_critical',\n","    'question_well_written',\n","    'question_multi_intent',\n","    'question_type_choice',\n","    'question_type_reason_explanation',\n","    'answer_type_reason_explanation',\n","]\n","\n","\n","# In[ ]:\n","model = QuestModel(n_classes=len(target_columns)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","\n","print(config.name)\n","for fold in range(config.fold):\n","    print('---%d-Fold---'%(fold+1))\n","    \n","    patience = 0\n","    best_loss   = 100.0\n","    best_score      = -1.\n","    best_preds = 0\n","    best_param_loss = None\n","    best_param_score = None\n","    \n","    for epoch in range(config.epochs):\n","        \n","        torch.cuda.empty_cache()\n","        start_time   = time.time()\n","        \n","        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n","        \n","        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n","        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n","        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n","        \n","\n","        if score_val > best_score:\n","            best_score = score_val\n","            best_param_score = model.state_dict()\n","            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","        else:\n","            patience += 1\n","            if patience >= config.patience:\n","                del train_loader, val_loader, loss_train, loss_val, score_val\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                break\n","    \n","        del train_loader, val_loader, loss_train, loss_val, score_val\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    model.load_state_dict(best_param_score)\n","    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","# train_loader = get_train_loader(batch_size=32)\n","# models = create_models()\n","# preds = predict(models, train_loader)\n","# train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","# cv_score = calc_spearman(train[target_columns].values, preds)\n","# print(cv_score)\n","\n","# del train_loader, models, preds, train\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}.txt'.format(config.name), 'w') as f:\n","    f.write('')\n","\n","\n","# ## AF\n","\n","# In[ ]:\n","\n","\n","config = PipeLineConfig(lr=1e-5,                         warmup=0.01,                         epochs=4,                         patience=3,                         batch_size=8,                         seed=42,                         name='reModel_AF',                         question_weight=0.5,                         answer_weight=0.5,                         fold=5,                         train=True\n","                       )\n","\n","\n","# In[ ]:\n","\n","\n","target_columns = [\n","    'question_asker_intent_understanding',\n","    'question_body_critical',\n","    'question_well_written',\n","    'question_type_instructions',\n","    'answer_type_instructions',\n","    'question_type_procedure',\n","    'answer_type_procedure',\n","]\n","\n","\n","# In[ ]:\n","model = QuestModel(n_classes=len(target_columns)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","\n","for fold in range(config.fold):\n","    print('---%d-Fold---'%(fold+1))\n","    \n","    patience = 0\n","    best_loss   = 100.0\n","    best_score      = -1.\n","    best_preds = 0\n","    best_param_loss = None\n","    best_param_score = None\n","    \n","    for epoch in range(config.epochs):\n","        \n","        torch.cuda.empty_cache()\n","        start_time   = time.time()\n","        \n","        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n","        \n","        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n","        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n","        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n","        \n","\n","        if score_val > best_score:\n","            best_score = score_val\n","            best_param_score = model.state_dict()\n","            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","        else:\n","            patience += 1\n","            if patience >= config.patience:\n","                del train_loader, val_loader, loss_train, loss_val, score_val\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                break\n","    \n","        del train_loader, val_loader, loss_train, loss_val, score_val\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    model.load_state_dict(best_param_score)\n","    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","# train_loader = get_train_loader(batch_size=32)\n","# models = create_models()\n","# preds = predict(models, train_loader)\n","# train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","# cv_score = calc_spearman(train[target_columns].values, preds)\n","# print(cv_score)\n","\n","# del train_loader, models, preds, train\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}.txt'.format(config.name), 'w') as f:\n","    f.write('')\n","\n","\n","# ## AG\n","\n","# In[ ]:\n","\n","\n","config = PipeLineConfig(lr=1e-5,                         warmup=0.01,                         epochs=4,                         patience=3,                         batch_size=8,                         seed=42,                         name='reModel_AG',                         question_weight=0.5,                         answer_weight=0.5,                         fold=5,                         train=True\n","                       )\n","\n","\n","# In[ ]:\n","\n","\n","target_columns = [\n","    'question_asker_intent_understanding',\n","    'question_body_critical',\n","    'question_well_written',\n","    'answer_helpful',\n","    'answer_level_of_information',\n","    'answer_plausible',\n","    'answer_relevance',\n","    'answer_satisfaction',\n","    'answer_well_written'\n","]\n","\n","\n","# In[ ]:\n","model = QuestModel(n_classes=len(target_columns)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","\n","print(config.name)\n","for fold in range(config.fold):\n","    print('---%d-Fold---'%(fold+1))\n","    \n","    patience = 0\n","    best_loss   = 100.0\n","    best_score      = -1.\n","    best_preds = 0\n","    best_param_loss = None\n","    best_param_score = None\n","    \n","    for epoch in range(config.epochs):\n","        \n","        torch.cuda.empty_cache()\n","        start_time   = time.time()\n","        \n","        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n","        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n","        \n","        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n","        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n","        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n","        \n","\n","        if score_val > best_score:\n","            best_score = score_val\n","            best_param_score = model.state_dict()\n","            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","        else:\n","            patience += 1\n","            if patience >= config.patience:\n","                del train_loader, val_loader, loss_train, loss_val, score_val\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                break\n","    \n","        del train_loader, val_loader, loss_train, loss_val, score_val\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        \n","    model.load_state_dict(best_param_score)\n","    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n","    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","# train_loader = get_train_loader(batch_size=32)\n","# models = create_models()\n","# preds = predict(models, train_loader)\n","# train = pd.read_csv(f'{DATA_DIR}/train.csv')\n","# cv_score = calc_spearman(train[target_columns].values, preds)\n","# print(cv_score)\n","\n","# del train_loader, models, preds, train\n","# torch.cuda.empty_cache()\n","# gc.collect()\n","\n","\n","# In[ ]:\n","\n","\n","with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}.txt'.format(config.name), 'w') as f:\n","    f.write('')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LakAo02RUVYk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}