{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, gc, random, multiprocessing, glob, time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#DATA_DIR = '../input/google-quest-challenge'\n",
    "DATA_DIR = 'D:/project/ICF_AutoCapsule_disabled/kaggle/google-quest-challenge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ../input/sacremoses/sacremoses-master/\n",
    "# !pip install ../input/transformers/transformers-master/\n",
    "\n",
    "# !pip install ../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n",
    "# !pip install ../input/glove840b300dtxt/glove.840B.300d.txt\n",
    "\n",
    "# CRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "# GLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "CRAWL_EMBEDDING_PATH = 'D:/project/ICF_AutoCapsule_disabled/W2V/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = 'D:/project/ICF_AutoCapsule_disabled/W2V/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make results reproducible .Else noone will believe you .\n",
    "import random\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, epochs, patience, batch_size, seed, name, question_weight,answer_weight,fold,train):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.name = name\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight =answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=20, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='test', \\\n",
    "                        question_weight=0.8, \\\n",
    "                        answer_weight=0.2, \\\n",
    "                        fold=3, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                              0.00308                 0.00308   \n",
       "1     46                              0.00448                 0.00448   \n",
       "2     70                              0.00673                 0.00673   \n",
       "3    132                              0.01401                 0.01401   \n",
       "4    200                              0.02074                 0.02074   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                  0.00308                       0.00308   \n",
       "1                  0.00448                       0.00448   \n",
       "2                  0.00673                       0.00673   \n",
       "3                  0.01401                       0.01401   \n",
       "4                  0.02074                       0.02074   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0                0.00308                                0.00308   \n",
       "1                0.00448                                0.00448   \n",
       "2                0.00673                                0.00673   \n",
       "3                0.01401                                0.01401   \n",
       "4                0.02074                                0.02074   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                          0.00308                        0.00308   \n",
       "1                          0.00448                        0.00448   \n",
       "2                          0.00673                        0.00673   \n",
       "3                          0.01401                        0.01401   \n",
       "4                          0.02074                        0.02074   \n",
       "\n",
       "   question_multi_intent  question_not_really_a_question  \\\n",
       "0                0.00308                         0.00308   \n",
       "1                0.00448                         0.00448   \n",
       "2                0.00673                         0.00673   \n",
       "3                0.01401                         0.01401   \n",
       "4                0.02074                         0.02074   \n",
       "\n",
       "   question_opinion_seeking  question_type_choice  question_type_compare  \\\n",
       "0                   0.00308               0.00308                0.00308   \n",
       "1                   0.00448               0.00448                0.00448   \n",
       "2                   0.00673               0.00673                0.00673   \n",
       "3                   0.01401               0.01401                0.01401   \n",
       "4                   0.02074               0.02074                0.02074   \n",
       "\n",
       "   question_type_consequence  question_type_definition  question_type_entity  \\\n",
       "0                    0.00308                   0.00308               0.00308   \n",
       "1                    0.00448                   0.00448               0.00448   \n",
       "2                    0.00673                   0.00673               0.00673   \n",
       "3                    0.01401                   0.01401               0.01401   \n",
       "4                    0.02074                   0.02074               0.02074   \n",
       "\n",
       "   question_type_instructions  question_type_procedure  \\\n",
       "0                     0.00308                  0.00308   \n",
       "1                     0.00448                  0.00448   \n",
       "2                     0.00673                  0.00673   \n",
       "3                     0.01401                  0.01401   \n",
       "4                     0.02074                  0.02074   \n",
       "\n",
       "   question_type_reason_explanation  question_type_spelling  \\\n",
       "0                           0.00308                 0.00308   \n",
       "1                           0.00448                 0.00448   \n",
       "2                           0.00673                 0.00673   \n",
       "3                           0.01401                 0.01401   \n",
       "4                           0.02074                 0.02074   \n",
       "\n",
       "   question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "0                0.00308         0.00308                      0.00308   \n",
       "1                0.00448         0.00448                      0.00448   \n",
       "2                0.00673         0.00673                      0.00673   \n",
       "3                0.01401         0.01401                      0.01401   \n",
       "4                0.02074         0.02074                      0.02074   \n",
       "\n",
       "   answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "0           0.00308           0.00308              0.00308   \n",
       "1           0.00448           0.00448              0.00448   \n",
       "2           0.00673           0.00673              0.00673   \n",
       "3           0.01401           0.01401              0.01401   \n",
       "4           0.02074           0.02074              0.02074   \n",
       "\n",
       "   answer_type_instructions  answer_type_procedure  \\\n",
       "0                   0.00308                0.00308   \n",
       "1                   0.00448                0.00448   \n",
       "2                   0.00673                0.00673   \n",
       "3                   0.01401                0.01401   \n",
       "4                   0.02074                0.02074   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                         0.00308              0.00308  \n",
       "1                         0.00448              0.00448  \n",
       "2                         0.00673              0.00673  \n",
       "3                         0.01401              0.01401  \n",
       "4                         0.02074              0.02074  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_asker_intent_understanding',\n",
       " 'question_body_critical',\n",
       " 'question_conversational',\n",
       " 'question_expect_short_answer',\n",
       " 'question_fact_seeking',\n",
       " 'question_has_commonly_accepted_answer',\n",
       " 'question_interestingness_others',\n",
       " 'question_interestingness_self',\n",
       " 'question_multi_intent',\n",
       " 'question_not_really_a_question',\n",
       " 'question_opinion_seeking',\n",
       " 'question_type_choice',\n",
       " 'question_type_compare',\n",
       " 'question_type_consequence',\n",
       " 'question_type_definition',\n",
       " 'question_type_entity',\n",
       " 'question_type_instructions',\n",
       " 'question_type_procedure',\n",
       " 'question_type_reason_explanation',\n",
       " 'question_type_spelling',\n",
       " 'question_well_written',\n",
       " 'answer_helpful',\n",
       " 'answer_level_of_information',\n",
       " 'answer_plausible',\n",
       " 'answer_relevance',\n",
       " 'answer_satisfaction',\n",
       " 'answer_type_instructions',\n",
       " 'answer_type_procedure',\n",
       " 'answer_type_reason_explanation',\n",
       " 'answer_well_written']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns = sub.columns.values[1:].tolist()\n",
    "target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>judaism.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>graphicdesign.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0      0  What am I losing when using extension tubes in...   \n",
       "1      1  What is the distinction between a city and a s...   \n",
       "2      2  Maximum protusion length for through-hole comp...   \n",
       "3      3              Can an affidavit be used in Beit Din?   \n",
       "4      5       How do you make a binary image in Photoshop?   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  After playing around with macro photography on...               ysap   \n",
       "1  I am trying to understand what kinds of places...      russellpierce   \n",
       "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4  I am trying to make a binary image. I want mor...            leigero   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1024   \n",
       "1           https://rpg.stackexchange.com/users/8774   \n",
       "2  https://electronics.stackexchange.com/users/10157   \n",
       "3       https://judaism.stackexchange.com/users/5151   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  I just got extension tubes, so here's the skin...           rfusca   \n",
       "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "\n",
       "                                    answer_user_page  \\\n",
       "0         https://photo.stackexchange.com/users/1917   \n",
       "1           https://rpg.stackexchange.com/users/1871   \n",
       "2  https://electronics.stackexchange.com/users/64754   \n",
       "3       https://judaism.stackexchange.com/users/4794   \n",
       "4  https://graphicdesign.stackexchange.com/users/...   \n",
       "\n",
       "                                                 url   category  \\\n",
       "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS   \n",
       "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE   \n",
       "2  http://electronics.stackexchange.com/questions...    SCIENCE   \n",
       "3  http://judaism.stackexchange.com/questions/551...    CULTURE   \n",
       "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS   \n",
       "\n",
       "                              host  question_asker_intent_understanding  \\\n",
       "0          photo.stackexchange.com                             1.000000   \n",
       "1            rpg.stackexchange.com                             1.000000   \n",
       "2    electronics.stackexchange.com                             0.888889   \n",
       "3        judaism.stackexchange.com                             0.888889   \n",
       "4  graphicdesign.stackexchange.com                             1.000000   \n",
       "\n",
       "   question_body_critical  question_conversational  \\\n",
       "0                0.333333                 0.000000   \n",
       "1                1.000000                 0.000000   \n",
       "2                0.666667                 0.000000   \n",
       "3                0.666667                 0.666667   \n",
       "4                0.666667                 0.000000   \n",
       "\n",
       "   question_expect_short_answer  question_fact_seeking  \\\n",
       "0                           0.0                    0.0   \n",
       "1                           0.5                    1.0   \n",
       "2                           1.0                    1.0   \n",
       "3                           1.0                    1.0   \n",
       "4                           1.0                    1.0   \n",
       "\n",
       "   question_has_commonly_accepted_answer  question_interestingness_others  \\\n",
       "0                                    0.0                         1.000000   \n",
       "1                                    1.0                         0.444444   \n",
       "2                                    1.0                         0.666667   \n",
       "3                                    1.0                         0.444444   \n",
       "4                                    1.0                         0.666667   \n",
       "\n",
       "   question_interestingness_self  question_multi_intent  \\\n",
       "0                       1.000000               0.000000   \n",
       "1                       0.444444               0.666667   \n",
       "2                       0.444444               0.333333   \n",
       "3                       0.444444               0.000000   \n",
       "4                       0.666667               0.000000   \n",
       "\n",
       "   question_not_really_a_question  question_opinion_seeking  \\\n",
       "0                             0.0                  1.000000   \n",
       "1                             0.0                  0.000000   \n",
       "2                             0.0                  0.333333   \n",
       "3                             0.0                  0.000000   \n",
       "4                             0.0                  0.000000   \n",
       "\n",
       "   question_type_choice  question_type_compare  question_type_consequence  \\\n",
       "0              0.000000               0.000000                        0.0   \n",
       "1              0.666667               0.666667                        0.0   \n",
       "2              0.000000               0.000000                        0.0   \n",
       "3              1.000000               0.000000                        0.0   \n",
       "4              0.000000               0.000000                        0.0   \n",
       "\n",
       "   question_type_definition  question_type_entity  question_type_instructions  \\\n",
       "0                  0.000000                   0.0                         1.0   \n",
       "1                  0.333333                   0.0                         0.0   \n",
       "2                  0.000000                   0.0                         1.0   \n",
       "3                  0.000000                   0.0                         0.0   \n",
       "4                  0.000000                   0.0                         1.0   \n",
       "\n",
       "   question_type_procedure  question_type_reason_explanation  \\\n",
       "0                 0.000000                          0.000000   \n",
       "1                 0.000000                          0.333333   \n",
       "2                 0.333333                          0.333333   \n",
       "3                 0.000000                          0.000000   \n",
       "4                 0.000000                          1.000000   \n",
       "\n",
       "   question_type_spelling  question_well_written  answer_helpful  \\\n",
       "0                     0.0               1.000000        1.000000   \n",
       "1                     0.0               0.888889        0.888889   \n",
       "2                     0.0               0.777778        0.777778   \n",
       "3                     0.0               0.888889        0.833333   \n",
       "4                     0.0               1.000000        1.000000   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.666667          1.000000          1.000000   \n",
       "1                     0.555556          0.888889          0.888889   \n",
       "2                     0.555556          1.000000          1.000000   \n",
       "3                     0.333333          0.833333          1.000000   \n",
       "4                     0.666667          1.000000          1.000000   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.800000                       1.0               0.000000   \n",
       "1             0.666667                       0.0               0.000000   \n",
       "2             0.666667                       0.0               0.333333   \n",
       "3             0.800000                       0.0               0.000000   \n",
       "4             0.800000                       1.0               0.000000   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.000000             1.000000  \n",
       "1                        0.666667             0.888889  \n",
       "2                        1.000000             0.888889  \n",
       "3                        1.000000             1.000000  \n",
       "4                        1.000000             1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>Will leaving corpses lying around upset my pri...</td>\n",
       "      <td>I see questions/information online about how t...</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>https://gaming.stackexchange.com/users/64471</td>\n",
       "      <td>There is no consequence for leaving corpses an...</td>\n",
       "      <td>Nelson868</td>\n",
       "      <td>https://gaming.stackexchange.com/users/97324</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/1979...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>Url link to feature image in the portfolio</td>\n",
       "      <td>I am new to Wordpress. i have issue with Featu...</td>\n",
       "      <td>Anu</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/72927</td>\n",
       "      <td>I think it is possible with custom fields.\\n\\n...</td>\n",
       "      <td>Irina</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/27233</td>\n",
       "      <td>http://wordpress.stackexchange.com/questions/1...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>wordpress.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>Is accuracy, recoil or bullet spread affected ...</td>\n",
       "      <td>To experiment I started a bot game, toggled in...</td>\n",
       "      <td>Konsta</td>\n",
       "      <td>https://gaming.stackexchange.com/users/37545</td>\n",
       "      <td>You do not have armour in the screenshots. Thi...</td>\n",
       "      <td>Damon Smithies</td>\n",
       "      <td>https://gaming.stackexchange.com/users/70641</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/2154...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>Suddenly got an I/O error from my external HDD</td>\n",
       "      <td>I have used my Raspberry Pi as a torrent-serve...</td>\n",
       "      <td>robbannn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/17341</td>\n",
       "      <td>Your Western Digital hard drive is disappearin...</td>\n",
       "      <td>HeatfanJohn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/1311</td>\n",
       "      <td>http://raspberrypi.stackexchange.com/questions...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>raspberrypi.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>Passenger Name - Flight Booking Passenger only...</td>\n",
       "      <td>I have bought Delhi-London return flights for ...</td>\n",
       "      <td>Amit</td>\n",
       "      <td>https://travel.stackexchange.com/users/29089</td>\n",
       "      <td>I called two persons who work for Saudia (tick...</td>\n",
       "      <td>Nean Der Thal</td>\n",
       "      <td>https://travel.stackexchange.com/users/10051</td>\n",
       "      <td>http://travel.stackexchange.com/questions/4704...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>travel.stackexchange.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0     39  Will leaving corpses lying around upset my pri...   \n",
       "1     46         Url link to feature image in the portfolio   \n",
       "2     70  Is accuracy, recoil or bullet spread affected ...   \n",
       "3    132     Suddenly got an I/O error from my external HDD   \n",
       "4    200  Passenger Name - Flight Booking Passenger only...   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  I see questions/information online about how t...              Dylan   \n",
       "1  I am new to Wordpress. i have issue with Featu...                Anu   \n",
       "2  To experiment I started a bot game, toggled in...             Konsta   \n",
       "3  I have used my Raspberry Pi as a torrent-serve...           robbannn   \n",
       "4  I have bought Delhi-London return flights for ...               Amit   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0       https://gaming.stackexchange.com/users/64471   \n",
       "1    https://wordpress.stackexchange.com/users/72927   \n",
       "2       https://gaming.stackexchange.com/users/37545   \n",
       "3  https://raspberrypi.stackexchange.com/users/17341   \n",
       "4       https://travel.stackexchange.com/users/29089   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  There is no consequence for leaving corpses an...        Nelson868   \n",
       "1  I think it is possible with custom fields.\\n\\n...            Irina   \n",
       "2  You do not have armour in the screenshots. Thi...   Damon Smithies   \n",
       "3  Your Western Digital hard drive is disappearin...      HeatfanJohn   \n",
       "4  I called two persons who work for Saudia (tick...    Nean Der Thal   \n",
       "\n",
       "                                   answer_user_page  \\\n",
       "0      https://gaming.stackexchange.com/users/97324   \n",
       "1   https://wordpress.stackexchange.com/users/27233   \n",
       "2      https://gaming.stackexchange.com/users/70641   \n",
       "3  https://raspberrypi.stackexchange.com/users/1311   \n",
       "4      https://travel.stackexchange.com/users/10051   \n",
       "\n",
       "                                                 url    category  \\\n",
       "0  http://gaming.stackexchange.com/questions/1979...     CULTURE   \n",
       "1  http://wordpress.stackexchange.com/questions/1...  TECHNOLOGY   \n",
       "2  http://gaming.stackexchange.com/questions/2154...     CULTURE   \n",
       "3  http://raspberrypi.stackexchange.com/questions...  TECHNOLOGY   \n",
       "4  http://travel.stackexchange.com/questions/4704...     CULTURE   \n",
       "\n",
       "                            host  \n",
       "0       gaming.stackexchange.com  \n",
       "1    wordpress.stackexchange.com  \n",
       "2       gaming.stackexchange.com  \n",
       "3  raspberrypi.stackexchange.com  \n",
       "4       travel.stackexchange.com  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokneizerの学習\n",
    "Tokenizer = keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "Tokenizer.fit_on_texts(list(train.question_title) + list(train.question_body) + list(train.answer) \\\n",
    "                        + list(test.question_title) + list(test.question_body) + list(test.answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category, host変数を、数値化(LabelEncode)\n",
    "le_category = LabelEncoder()\n",
    "le_category.fit(np.concatenate([train.category.values, test.category.values], axis=0))\n",
    "train['category'] = le_category.transform(train.category)\n",
    "test['category'] = le_category.transform(test.category)\n",
    "\n",
    "le_host = LabelEncoder()\n",
    "le_host.fit(np.concatenate([train.host.values, test.host.values], axis=0))\n",
    "train['host'] = le_host.transform(train.host)\n",
    "test['host'] = le_host.transform(test.host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_MAX_LEN = 50\n",
    "BODY_MAX_LEN = 500\n",
    "ANSWER_MAX_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestDataset(torch.utils.data.TensorDataset):\n",
    "\n",
    "    def __init__(self, df, train_mode=True, labeled=True):\n",
    "        self.df = df\n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "        self.tokenizer = Tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        token_id列\n",
    "        label列\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        title_ids, question_ids, answer_ids = self.get_token_ids(row)\n",
    "\n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return title_ids, question_ids, answer_ids, torch.tensor(row.category), torch.tensor(row.host), labels\n",
    "        else:\n",
    "            return title_ids, question_ids, answer_ids, torch.tensor(row.category), torch.tensor(row.host)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def get_token_ids(self, row):\n",
    "        t = self.tokenizer.texts_to_sequences([title])\n",
    "        q = self.tokenizer.texts_to_sequences([question])\n",
    "        a = self.tokenizer.texts_to_sequences([answer])\n",
    "        \n",
    "        t = sequence.pad_sequences(t, maxlen=TITLE_MAX_LEN, padding='post')\n",
    "        q = sequence.pad_sequences(q, maxlen=BODY_MAX_LEN, padding='post')\n",
    "        a = sequence.pad_sequences(a, maxlen=ANSWER_MAX_LEN, padding='post')\n",
    "\n",
    "        return torch.tensor(t), torch.tensor(q), torch.tensor(a)\n",
    "    \n",
    "    \n",
    "    def get_label(self, row):\n",
    "        return torch.tensor(row[target_columns].values.astype(np.float32))\n",
    "    \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        labelデータを持つモードと、ない完全な推論モードでは、batchのshapeが異なるので(labelが2番目の要素にあるなし)\n",
    "        \"\"\"\n",
    "        title_ids = torch.stack([x[0] for x in batch])\n",
    "        question_ids = torch.stack([x[1] for x in batch])\n",
    "        answer_ids = torch.stack([x[2] for x in batch])\n",
    "        category = torch.stack([x[3] for x in batch])\n",
    "        host = torch.stack([x[4] for x in batch])\n",
    "    \n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[-1] for x in batch])\n",
    "            return title_ids, question_ids, answer_ids, category, host, labels\n",
    "        else:\n",
    "            return title_ids, question_ids, answer_ids, category, host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n",
    "    df = train.copy()\n",
    "    df = shuffle(df, random_state=1234)\n",
    "    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold == ifold:\n",
    "            df_train = df.iloc[train_idx]\n",
    "            df_val = df.iloc[valid_idx]\n",
    "            break\n",
    "\n",
    "    print('train', df_train.shape)\n",
    "    print('val', df_val.shape)\n",
    "\n",
    "    ds_train = QuestDataset(df_train, train_mode=True)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n",
    "    train_loader.num = len(df_train)\n",
    "\n",
    "    ds_val = QuestDataset(df_val, train_mode=False)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n",
    "    val_loader.num = len(df_val)\n",
    "    val_loader.df = df_val\n",
    "\n",
    "    return train_loader, val_loader, df_val.shape[0]\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=4):\n",
    "    df = test.copy()\n",
    "    df_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(df_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['2000000', '300']\n",
      "301\n",
      "[',', '-0.0282', '-0.0557', '-0.0451', '-0.0434', '0.0712', '-0.0855', '-0.1085', '-0.0561', '-0.4523', '-0.0202', '0.0975', '0.1047', '0.1962', '-0.0693', '0.0213', '-0.0235', '0.1336', '-0.0420', '-0.0564', '-0.0798', '0.0424', '-0.0409', '-0.0536', '-0.0252', '0.0135', '0.0064', '0.1235', '0.0461', '0.0120', '-0.0372', '0.0650', '0.0041', '-0.1074', '-0.0263', '0.1133', '-0.0029', '0.0671', '0.1065', '0.0234', '-0.0160', '0.0070', '0.4355', '-0.0752', '-0.4328', '0.0457', '0.0604', '-0.0740', '-0.0055', '-0.0089', '-0.2926', '-0.0545', '-0.1519', '0.0990', '-0.0193', '-0.0050', '0.0511', '0.0404', '0.1023', '-0.0128', '0.0488', '-0.1567', '-0.0759', '-0.0190', '0.1442', '0.0047', '-0.0186', '0.0140', '-0.0385', '-0.0853', '0.1572', '0.1770', '0.0084', '-0.0250', '-0.1145', '-0.0663', '-0.1244', '-0.3977', '-0.0124', '-0.4586', '-0.0220', '0.5746', '0.0218', '-0.0754', '0.0099', '0.0397', '-0.0154', '0.0424', '-0.0150', '-0.0016', '0.0305', '0.0101', '0.2266', '0.1394', '0.0189', '0.0069', '0.0394', '0.0355', '-0.0111', '-0.0687', '-0.0078', '0.0224', '0.0817', '-0.1949', '0.0001', '0.4047', '-0.0237', '-0.0656', '-0.0684', '0.0233', '0.0438', '0.1203', '-0.0276', '0.0416', '0.0114', '-0.4529', '0.1538', '0.1323', '-0.0186', '-0.0914', '-0.0312', '0.1051', '0.0212', '0.0798', '-0.0104', '-0.0206', '-0.0025', '0.0043', '-0.0378', '0.2689', '0.0747', '-0.0418', '-0.0048', '-0.0387', '0.0432', '0.1704', '0.0614', '0.0905', '-0.0436', '-0.0141', '-0.0315', '0.0276', '0.0151', '-0.0103', '-0.0266', '-0.0512', '-0.0408', '-0.0651', '0.0662', '-0.0936', '0.1371', '0.0458', '-0.1366', '-0.0075', '-0.0104', '-0.0732', '0.1205', '0.1035', '0.0106', '-0.0317', '-0.0316', '0.6639', '-0.0022', '-0.1343', '0.0144', '-0.0338', '0.0034', '-0.0429', '-0.0821', '0.0037', '0.1029', '-0.0204', '-0.0269', '0.0052', '-0.1034', '0.1068', '0.0121', '0.0980', '-0.0458', '0.0199', '-0.0132', '0.1936', '-0.0213', '0.0209', '-0.0025', '0.0416', '-0.0337', '0.0516', '-0.1014', '0.0203', '0.0198', '-0.0305', '-0.0313', '0.0543', '-0.0106', '0.1441', '-0.0178', '-0.0627', '0.0475', '0.0352', '-0.0254', '-0.0949', '0.0401', '0.0317', '0.0055', '-0.0536', '0.0191', '-0.0511', '-0.0409', '-0.0030', '0.1582', '0.0108', '0.5237', '0.0436', '0.0306', '-0.0392', '0.0177', '0.0069', '0.0605', '0.1206', '-0.0216', '-0.0633', '-0.2965', '0.0521', '-0.0150', '-0.2207', '-0.0642', '-0.0906', '-0.0121', '0.0569', '0.0944', '-0.0652', '-0.0108', '-0.0477', '0.0023', '0.0077', '-0.1547', '0.0463', '0.0698', '-0.0376', '-0.0291', '0.0033', '-0.0102', '-0.0743', '0.0085', '0.0805', '-0.0291', '-0.0674', '-0.0586', '-0.0653', '0.0283', '-0.0255', '0.0869', '-0.0868', '0.0090', '0.3245', '-0.0573', '-0.0289', '0.0470', '-0.0117', '0.0174', '0.0132', '-0.0226', '-0.0664', '0.0188', '0.0263', '0.0111', '-0.0049', '-0.0656', '0.0295', '0.0435', '0.0290', '0.1163', '0.0448', '-0.1139', '-0.0553', '-0.0528', '0.1745', '-0.0146', '-0.1308', '-0.0607', '-0.0134', '0.0781', '0.0378', '0.0228', '-0.0728', '-0.0059', '0.0158', '-0.0141', '-0.0002', '0.0193', '-0.0148', '-0.0463', '0.0444', '0.3034', '0.1020', '-0.0871', '0.0317', '-0.0370', '-0.0725', '-0.0042']\n"
     ]
    }
   ],
   "source": [
    "with open(CRAWL_EMBEDDING_PATH, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i<2:\n",
    "            #print(line)\n",
    "            print(len(line.strip().split(' ')))\n",
    "            print(line.strip().split(' '))\n",
    "        else:\n",
    "            break\n",
    "    #return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    #embedding_index = load_embeddings(path)\n",
    "    embedding_index = KeyedVectors.load(path)\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-2c2d41f09f48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrawl_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munknown_words_crawl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCRAWL_EMBEDDING_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'n unknown words (crawl): '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknown_words_crawl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(Tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [02:46, 13207.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  142634\n"
     ]
    }
   ],
   "source": [
    "glove_matrix, unknown_words_glove = build_matrix(Tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)  # 600次元のベクトルに\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class SpatialDropout(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super(SpatialDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)   # convert to [batch, feature, timestep]\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)   # back to [batch, timestep, feature]\n",
    "        return x\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_unit, num_layer=1):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.max_feature = embedding_matrix.shape[0]\n",
    "        self.embedding_size = embedding_matrix.shape[1]\n",
    "      \n",
    "        self.embedding_body = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_body.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_body.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_answer = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_answer.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_answer.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_title = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_title.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_title.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_dropout = SpatialDropout(0.4)\n",
    "        \n",
    "        self.lstm1_body = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_body = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm1_answer = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_answer = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm1_title = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_title = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.attention_body = Attention(hidden_unit, BODY_MAX_LEN)\n",
    "        self.attention_answer = Attention(hidden_unit, ANSWER_MAX_LEN)\n",
    "        self.attention_title = Attention(hidden_unit, TITLE_MAX_LEN)\n",
    "        \n",
    "#         self.category = nn.Embedding(5, 10)\n",
    "#         self.host = nn.Embedding(64, 128)\n",
    "        \n",
    "        self.linear_title = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        self.linear_body = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        self.linear_answer = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        \n",
    "#         self.linear_out = nn.Linear(hidden_unit, 30)\n",
    "        self.additional_category = nn.Linear(5, 5)\n",
    "        self.additional_host = nn.Linear(64, 32)\n",
    "        \n",
    "        self.linear_q = nn.Linear(hidden_unit*2+37, hidden_unit)\n",
    "        self.linear_a = nn.Linear(hidden_unit+37, hidden_unit)\n",
    "        self.linear_q_out = nn.Linear(hidden_unit, 21)\n",
    "        self.linear_a_out = nn.Linear(hidden_unit, 9)\n",
    "        \n",
    "    def forward(self, body, answer, title, category, host):\n",
    "        \n",
    "        x_body = self.embedding_dropout(self.embedding_body(body))\n",
    "        h_lstm1_body, _ = self.lstm1_body(x_body)\n",
    "        h_lstm2_body, _ = self.lstm2_body(h_lstm1_body)\n",
    "        \n",
    "        x_answer = self.embedding_dropout(self.embedding_answer(answer))\n",
    "        h_lstm1_answer, _ = self.lstm1_answer(x_answer)\n",
    "        h_lstm2_answer, _ = self.lstm2_answer(h_lstm1_answer)\n",
    "        \n",
    "        x_title = self.embedding_dropout(self.embedding_title(title))\n",
    "        h_lstm1_title, _ = self.lstm1_title(x_title)\n",
    "        h_lstm2_title, _ = self.lstm2_title(h_lstm1_title)\n",
    "        \n",
    "#         print(h_lstm2_body.size())\n",
    "        att_body = self.attention_body(h_lstm2_body)\n",
    "        att_answer = self.attention_answer(h_lstm2_answer)\n",
    "        att_title = self.attention_title(h_lstm2_title)\n",
    "        \n",
    "        avg_pool_body = torch.mean(h_lstm2_body, 1)\n",
    "        max_pool_body, _ = torch.max(h_lstm2_body, 1)\n",
    "        \n",
    "        avg_pool_answer = torch.mean(h_lstm2_answer, 1)\n",
    "        max_pool_answer, _ = torch.max(h_lstm2_answer, 1)\n",
    "        \n",
    "        avg_pool_title = torch.mean(h_lstm2_title, 1)\n",
    "        max_pool_title, _ = torch.max(h_lstm2_title, 1)\n",
    "        \n",
    "        body_cat = torch.cat((att_body, avg_pool_body, max_pool_body), 1)\n",
    "        answer_cat = torch.cat((att_answer, avg_pool_answer, max_pool_answer), 1)\n",
    "        title_cat = torch.cat((att_title, avg_pool_title, max_pool_title), 1)\n",
    "        \n",
    "#         additional_feature = self.addtional_linear()\n",
    "\n",
    "#         category = self.category(category)\n",
    "#         host = self.category(host)\n",
    "        \n",
    "        \n",
    "        body_cat = torch.relu(self.linear_body(body_cat))\n",
    "        answer_cat = torch.relu(self.linear_answer(answer_cat))\n",
    "        title_cat = torch.relu(self.linear_title(title_cat))\n",
    "\n",
    "        category = self.additional_category(category)\n",
    "        host = self.additional_host(host)\n",
    "        \n",
    "        hidden_q = self.linear_q(torch.cat((title_cat, body_cat, category, host), 1))\n",
    "        hidden_a = self.linear_a(torch.cat((answer_cat, category, host), 1))\n",
    "                                          \n",
    "        q_result = self.linear_q_out(hidden_q)\n",
    "        a_result = self.linear_a_out(hidden_a)\n",
    "        \n",
    "        out = torch.cat([q_result, a_result], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "    df = shuffle(df, random_state=1234)\n",
    "    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold == ifold:\n",
    "            df_train = df.iloc[train_idx]\n",
    "            df_val = df.iloc[valid_idx]\n",
    "            break\n",
    "\n",
    "    print('train', df_train.shape)\n",
    "    print('val', df_val.shape)\n",
    "\n",
    "    ds_train = QuestDataset(df_train, train_mode=True)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n",
    "    train_loader.num = len(df_train)\n",
    "\n",
    "    ds_val = QuestDataset(df_val, train_mode=False)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n",
    "    val_loader.num = len(df_val)\n",
    "    val_loader.df = df_val\n",
    "\n",
    "    return train_loader, val_loader, df_val.shape[0]\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=4):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "    df_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(df_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, optimizer, criterion, scheduler):\n",
    "    model.train()\n",
    "    avg_loss = 0.    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        title_ids_train, question_ids_train, answer_ids_train, category_train, host_train, labels_train = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device), batch[5].to(device)\n",
    "        \n",
    "        logits = model(body=question_ids_train, answer=answer_ids_train, title=title_ids_train, category=category_train, host=host_train)\n",
    "        \n",
    "        loss = config.question_weight*criterion(logits[:,0:21], labels_train[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], labels_train[:,21:30])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        del ids_train, seg_ids_train, label_ids_train\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss\n",
    "\n",
    "def val_model(val_loader, val_length, batch_size=8):\n",
    "    model.eval() # eval mode  \n",
    "    avg_val_loss = 0.\n",
    "    \n",
    "    valid_preds = np.zeros((val_length, len(target_columns)))\n",
    "    original = np.zeros((val_length, len(target_columns)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            title_ids_val, question_ids_val, answer_ids_val, category_val, host_val, labels_val = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "            \n",
    "            logits = torch.sigmoid(model(body=question_ids_val, answer=answer_ids_val, title=title_ids_val, category=category_val, host=host_val))\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels_val).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.tensor(valid_preds).numpy()\n",
    "        #preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        # np.save(\"preds.npy\", preds)\n",
    "        # np.save(\"actuals.npy\", original)\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_columns)):\n",
    "            print(i, spearmanr(original[:,i], preds[:,i]))\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "    \n",
    "    return avg_val_loss, score/len(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCUM_STEPS = 1\n",
    "HIDDEN_UNIT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 426.00 MiB (GPU 0; 4.00 GiB total capacity; 852.00 MiB already allocated; 25.56 MiB free; 0 bytes cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-301c62436b1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHIDDEN_UNIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\0000011306852\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\0000011306852\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\0000011306852\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\0000011306852\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 0; 4.00 GiB total capacity; 852.00 MiB already allocated; 25.56 MiB free; 0 bytes cached)"
     ]
    }
   ],
   "source": [
    "model = LSTM_Model(embedding_matrix, HIDDEN_UNIT).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "#         if loss_val < best_loss:\n",
    "#             patience = 0\n",
    "#             best_loss = loss_val \n",
    "#             best_param_loss = model.state_dict()\n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_lstm_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, 'best_param_lstm_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                break\n",
    "        \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_lstm_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, 'best_param_lstm_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "    \n",
    "    del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
