{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "6T6HUtB5eorj",
    "outputId": "b4aa974a-396d-41ff-f8ae-f7554b265e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhYQVdgEwuRZ"
   },
   "source": [
    "## やったこと\n",
    "\n",
    "- テキストのクリーニング処理 -> 改善\n",
    "- host, categoryのカテゴリカル変数をエンベディングして入力 -> 改善\n",
    "\n",
    "- epochs=20で、early-stoppingはあまり良くならなかった -> とりあえず速く数を回したいので、epochs=4でやっている\n",
    "\n",
    "\n",
    "- batch_size=8以上にすると、out_of_memoryになる\n",
    "\n",
    "- MSELossを使用 -> 悪化\n",
    "- titleは分けて、別のエンベディングとして入力 -> 悪化\n",
    "\n",
    "\n",
    "\n",
    "- BERTを2つ使う -> gpu不足\n",
    "- クラス分類問題にする（30*num_class） -> 学習が安定しない（nan）\n",
    "- 30個の目的変数それぞれ独立に予測するモデル -> 約30時間必要、あまり精度が出ないように見える -> 関連する目的変数だけをグルーピングしてモデルを分ける必要？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvK9IDYaOP_b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, gc, random, multiprocessing, glob, time\n",
    "\n",
    "DATA_DIR = '/content/drive/My Drive/Colab Notebooks/GoogleQuest/input/google-quest-challenge'\n",
    "# DATA_DIR = '../input/google-quest-challenge'\n",
    "# DATA_DIR = 'D:/project/ICF_AutoCapsule_disabled/kaggle/google-quest-challenge'\n",
    "# BERT_DIR = 'D:/project/ICF_AutoCapsule_disabled/BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4jYDD13OP_o"
   },
   "outputs": [],
   "source": [
    "# !pip install ../input/sacremoses/sacremoses-master/\n",
    "# !pip install ../input/transformers/transformers-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "xst4_Y1Xer4Y",
    "outputId": "ebce9329-b65c-4aed-bdf2-c8936164681e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: flashtext in /usr/local/lib/python3.6/dist-packages (2.7)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "OW9r1DNjOP_z",
    "outputId": "74de0aef-cdaf-49a2-a283-cedd06fbac25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQsu1MpOOQAA"
   },
   "outputs": [],
   "source": [
    "## Make results reproducible .Else noone will believe you .\n",
    "import random\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kegkamk4OQAO"
   },
   "outputs": [],
   "source": [
    "class PipeLineConfig:\n",
    "    def __init__(self, lr, warmup, epochs, patience, batch_size, seed, name, question_weight,answer_weight,fold,train):\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.name = name\n",
    "        self.question_weight = question_weight\n",
    "        self.answer_weight =answer_weight\n",
    "        self.fold = fold\n",
    "        self.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBzmdbwYOQAY"
   },
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=4, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='reModel_AB10', \\\n",
    "                        question_weight=0.5, \\\n",
    "                        answer_weight=0.5, \\\n",
    "                        fold=5, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBjN2cdKOQAh"
   },
   "outputs": [],
   "source": [
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DjOYABYwOQAr",
    "outputId": "c320e0a7-6319-4d6d-a33b-35d81c9ae868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "RrECgPr6OQA4",
    "outputId": "6160c4a7-976f-482d-ee6e-35bfc26359b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.00308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>0.00673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "      <td>0.01401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.02074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  ...  answer_well_written\n",
       "0     39  ...              0.00308\n",
       "1     46  ...              0.00448\n",
       "2     70  ...              0.00673\n",
       "3    132  ...              0.01401\n",
       "4    200  ...              0.02074\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "colab_type": "code",
    "id": "FUZFPsyDOQBB",
    "outputId": "409717bf-ff76-4f79-ee57-8d824da37d07",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_asker_intent_understanding',\n",
       " 'question_body_critical',\n",
       " 'question_conversational',\n",
       " 'question_expect_short_answer',\n",
       " 'question_fact_seeking',\n",
       " 'question_has_commonly_accepted_answer',\n",
       " 'question_interestingness_others',\n",
       " 'question_interestingness_self',\n",
       " 'question_multi_intent',\n",
       " 'question_not_really_a_question',\n",
       " 'question_opinion_seeking',\n",
       " 'question_type_choice',\n",
       " 'question_type_compare',\n",
       " 'question_type_consequence',\n",
       " 'question_type_definition',\n",
       " 'question_type_entity',\n",
       " 'question_type_instructions',\n",
       " 'question_type_procedure',\n",
       " 'question_type_reason_explanation',\n",
       " 'question_type_spelling',\n",
       " 'question_well_written',\n",
       " 'answer_helpful',\n",
       " 'answer_level_of_information',\n",
       " 'answer_plausible',\n",
       " 'answer_relevance',\n",
       " 'answer_satisfaction',\n",
       " 'answer_type_instructions',\n",
       " 'answer_type_procedure',\n",
       " 'answer_type_reason_explanation',\n",
       " 'answer_well_written']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns = sub.columns.values[1:].tolist()\n",
    "target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1M4MtN00v6H"
   },
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_well_written',\n",
    "        'question_conversational',\n",
    "        'question_opinion_seeking',\n",
    "        'question_not_really_a_question'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "BOiPYRDWOQBM",
    "outputId": "11755d14-b2bb-4e66-8842-59a4ab5b5cab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>question_not_really_a_question</th>\n",
       "      <th>question_opinion_seeking</th>\n",
       "      <th>question_type_choice</th>\n",
       "      <th>question_type_compare</th>\n",
       "      <th>question_type_consequence</th>\n",
       "      <th>question_type_definition</th>\n",
       "      <th>question_type_entity</th>\n",
       "      <th>question_type_instructions</th>\n",
       "      <th>question_type_procedure</th>\n",
       "      <th>question_type_reason_explanation</th>\n",
       "      <th>question_type_spelling</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>photo.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>rpg.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>electronics.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>judaism.stackexchange.com</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>graphicdesign.stackexchange.com</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  ... answer_well_written\n",
       "0      0  ...            1.000000\n",
       "1      1  ...            0.888889\n",
       "2      2  ...            0.888889\n",
       "3      3  ...            1.000000\n",
       "4      5  ...            1.000000\n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "id": "khOj2PbNOQBW",
    "outputId": "35758b0f-d24e-4b63-d1d5-1f6aaf1b276a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Will leaving corpses lying around upset my pri...</td>\n",
       "      <td>I see questions/information online about how t...</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>https://gaming.stackexchange.com/users/64471</td>\n",
       "      <td>There is no consequence for leaving corpses an...</td>\n",
       "      <td>Nelson868</td>\n",
       "      <td>https://gaming.stackexchange.com/users/97324</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/1979...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Url link to feature image in the portfolio</td>\n",
       "      <td>I am new to Wordpress. i have issue with Featu...</td>\n",
       "      <td>Anu</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/72927</td>\n",
       "      <td>I think it is possible with custom fields.\\n\\n...</td>\n",
       "      <td>Irina</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/27233</td>\n",
       "      <td>http://wordpress.stackexchange.com/questions/1...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>wordpress.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>Is accuracy, recoil or bullet spread affected ...</td>\n",
       "      <td>To experiment I started a bot game, toggled in...</td>\n",
       "      <td>Konsta</td>\n",
       "      <td>https://gaming.stackexchange.com/users/37545</td>\n",
       "      <td>You do not have armour in the screenshots. Thi...</td>\n",
       "      <td>Damon Smithies</td>\n",
       "      <td>https://gaming.stackexchange.com/users/70641</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/2154...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>Suddenly got an I/O error from my external HDD</td>\n",
       "      <td>I have used my Raspberry Pi as a torrent-serve...</td>\n",
       "      <td>robbannn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/17341</td>\n",
       "      <td>Your Western Digital hard drive is disappearin...</td>\n",
       "      <td>HeatfanJohn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/1311</td>\n",
       "      <td>http://raspberrypi.stackexchange.com/questions...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>raspberrypi.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>Passenger Name - Flight Booking Passenger only...</td>\n",
       "      <td>I have bought Delhi-London return flights for ...</td>\n",
       "      <td>Amit</td>\n",
       "      <td>https://travel.stackexchange.com/users/29089</td>\n",
       "      <td>I called two persons who work for Saudia (tick...</td>\n",
       "      <td>Nean Der Thal</td>\n",
       "      <td>https://travel.stackexchange.com/users/10051</td>\n",
       "      <td>http://travel.stackexchange.com/questions/4704...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>travel.stackexchange.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  ...                           host\n",
       "0     39  ...       gaming.stackexchange.com\n",
       "1     46  ...    wordpress.stackexchange.com\n",
       "2     70  ...       gaming.stackexchange.com\n",
       "3    132  ...  raspberrypi.stackexchange.com\n",
       "4    200  ...       travel.stackexchange.com\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUQkeCkGOQBi"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kw_iafZpOQBl"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf7fudYqOQBu"
   },
   "outputs": [],
   "source": [
    "PUNCTS = {\n",
    "            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n",
    "            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n",
    "            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n",
    "            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n",
    "            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n",
    "            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n",
    "            '↑', 'º', '¯', '♫', '#'\n",
    "          }\n",
    "\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n",
    "\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "kp = KeywordProcessor(case_sensitive=True)\n",
    "for k, v in mispell_dict.items():\n",
    "    kp.add_keyword(k, v)\n",
    "\n",
    "def clean_punct(text):\n",
    "    text = str(text)\n",
    "    for punct in PUNCTS:\n",
    "        text = text.replace(punct, ' {} '.format(punct))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\&lt)|(\\&gt)', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n",
    "    text = kp.replace_keywords(text)\n",
    "    text = clean_punct(text)\n",
    "    text = re.sub(r'\\n\\r', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBg42eTrfpzH"
   },
   "outputs": [],
   "source": [
    "category_list = ['CULTURE', 'LIFE_ARTS', 'SCIENCE', 'STACKOVERFLOW', 'TECHNOLOGY']\n",
    "\n",
    "host_list = ['academia.stackexchange.com', 'android.stackexchange.com',\n",
    "       'anime.stackexchange.com', 'apple.stackexchange.com',\n",
    "       'askubuntu.com', 'bicycles.stackexchange.com',\n",
    "       'biology.stackexchange.com', 'blender.stackexchange.com',\n",
    "       'boardgames.stackexchange.com', 'chemistry.stackexchange.com',\n",
    "       'christianity.stackexchange.com', 'codereview.stackexchange.com',\n",
    "       'cooking.stackexchange.com', 'crypto.stackexchange.com',\n",
    "       'cs.stackexchange.com', 'dba.stackexchange.com',\n",
    "       'diy.stackexchange.com', 'drupal.stackexchange.com',\n",
    "       'dsp.stackexchange.com', 'electronics.stackexchange.com',\n",
    "       'ell.stackexchange.com', 'english.stackexchange.com',\n",
    "       'expressionengine.stackexchange.com', 'gamedev.stackexchange.com',\n",
    "       'gaming.stackexchange.com', 'gis.stackexchange.com',\n",
    "       'graphicdesign.stackexchange.com', 'judaism.stackexchange.com',\n",
    "       'magento.stackexchange.com', 'math.stackexchange.com',\n",
    "       'mathematica.stackexchange.com', 'mathoverflow.net',\n",
    "       'mechanics.stackexchange.com', 'meta.askubuntu.com',\n",
    "       'meta.christianity.stackexchange.com',\n",
    "       'meta.codereview.stackexchange.com', 'meta.math.stackexchange.com',\n",
    "       'meta.stackexchange.com', 'meta.superuser.com',\n",
    "       'money.stackexchange.com', 'movies.stackexchange.com',\n",
    "       'music.stackexchange.com', 'photo.stackexchange.com',\n",
    "       'physics.stackexchange.com', 'programmers.stackexchange.com',\n",
    "       'raspberrypi.stackexchange.com', 'robotics.stackexchange.com',\n",
    "       'rpg.stackexchange.com', 'salesforce.stackexchange.com',\n",
    "       'scifi.stackexchange.com', 'security.stackexchange.com',\n",
    "       'serverfault.com', 'sharepoint.stackexchange.com',\n",
    "       'softwarerecs.stackexchange.com', 'stackoverflow.com',\n",
    "       'stats.stackexchange.com', 'superuser.com',\n",
    "       'tex.stackexchange.com', 'travel.stackexchange.com',\n",
    "       'unix.stackexchange.com', 'ux.stackexchange.com',\n",
    "       'webapps.stackexchange.com', 'webmasters.stackexchange.com',\n",
    "       'wordpress.stackexchange.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLL4EgrdOQCV"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4Kqfs4yOQCX"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "MAX_T_LEN = 30\n",
    "MAX_Q_LEN = 512-30-3\n",
    "SEP_TOKEN_ID = 102 # bert-base-uncasedにおけるvocabの'[SEP']が、102番目という意味\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True):\n",
    "        self.df = df\n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(BERT_DIR+'/bert-base-uncased')\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased/')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        token_id列\n",
    "        segment_id列\n",
    "        label列\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids, seg_ids = self.get_token_ids(row)\n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host), labels\n",
    "        else:\n",
    "            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "#     def select_tokens(self, tokens, max_num):\n",
    "#         if len(tokens) <= max_num:\n",
    "#             return tokens\n",
    "#         if self.train_mode:\n",
    "#             num_remove = len(tokens) - max_num\n",
    "#             remove_start = random.randint(0, len(tokens)-num_remove-1)\n",
    "#             return tokens[:remove_start] + tokens[remove_start + num_remove:]\n",
    "#         else:\n",
    "#             return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n",
    "\n",
    "    def trim_input(self, title, question, max_sequence_length=MAX_LEN, \n",
    "                t_max_len=MAX_T_LEN, q_max_len=MAX_Q_LEN):\n",
    "        \"\"\"\n",
    "        title. question, answerそれぞれのセンテンスを、tokenizeする\n",
    "        max_lengthに足りない分は、\n",
    "        \"\"\"\n",
    "        t = self.tokenizer.tokenize(title)\n",
    "        q = self.tokenizer.tokenize(question)\n",
    "\n",
    "\n",
    "        t = t[:t_max_len]\n",
    "        q = q[:q_max_len]\n",
    "\n",
    "        if len(t) < MAX_T_LEN:\n",
    "            \"\"\"0で後ろからpadding\"\"\"\n",
    "            t += [0] * (MAX_T_LEN - len(t))\n",
    "\n",
    "        if len(q) < MAX_Q_LEN:\n",
    "            \"\"\"0で後ろからpadding\"\"\"\n",
    "            q += [0] * (MAX_Q_LEN - len(q))\n",
    "\n",
    "        return t, q\n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        t_tokens, q_tokens = self.trim_input(row.question_title, row.question_body)\n",
    "        \n",
    "        # BERTの入力タイプに変換([CLS]と[SEP]をつないで、１つのsetentenceに)\n",
    "        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]']\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ids = torch.tensor(token_ids)\n",
    "        seg_ids = self.get_seg_ids(ids)  # segment_embを区別するindex\n",
    "        return ids, seg_ids\n",
    "    \n",
    "    def get_seg_ids(self, ids):\n",
    "        \"\"\"\n",
    "        いくつめの文かを区別するsegment_idを、各文字に振る\n",
    "        \"\"\"\n",
    "        seg_ids = torch.zeros_like(ids) # [max_len]のtorch_tensor\n",
    "        seg_idx = 0\n",
    "        first_sep = True\n",
    "        for i, e in enumerate(ids):\n",
    "            seg_ids[i] = seg_idx\n",
    "            if e == SEP_TOKEN_ID: # [SEP]の場合\n",
    "                seg_idx = 1\n",
    "        pad_idx = torch.nonzero(ids == 0)  # bert-base_uncasedのvocabで、[PAD]は0番目であるので、PADの部分のindexだけ抽出\n",
    "        seg_ids[pad_idx] = 0\n",
    "\n",
    "        return seg_ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "        #print(row[target_columns].values)\n",
    "        return torch.tensor(row[target_columns].values.astype(np.float32))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        labelデータを持つモードと、ない完全な推論モードでは、batchのshapeが異なるので(labelが2番目の要素にあるなし)\n",
    "        \"\"\"\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        seg_ids = torch.stack([x[1] for x in batch])\n",
    "        category = torch.stack([x[2] for x in batch])\n",
    "        host = torch.stack([x[3] for x in batch])\n",
    "    \n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[-1] for x in batch])\n",
    "            return token_ids, seg_ids, category, host, labels\n",
    "        else:\n",
    "            return token_ids, seg_ids, category, host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NXBQo6aOQCc"
   },
   "outputs": [],
   "source": [
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    df = shuffle(df, random_state=1234)\n",
    "    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold == ifold:\n",
    "            df_train = df.iloc[train_idx]\n",
    "            df_val = df.iloc[valid_idx]\n",
    "            break\n",
    "\n",
    "    print('train', df_train.shape)\n",
    "    print('val', df_val.shape)\n",
    "\n",
    "    ds_train = QuestDataset(df_train, train_mode=True)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n",
    "    train_loader.num = len(df_train)\n",
    "\n",
    "    ds_val = QuestDataset(df_val, train_mode=False)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n",
    "    val_loader.num = len(df_val)\n",
    "    val_loader.df = df_val\n",
    "\n",
    "    return train_loader, val_loader, df_val.shape[0]\n",
    "\n",
    "\n",
    "def get_train_loader(batch_size=4):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=4):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    #df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7p98IBtOQCl"
   },
   "outputs": [],
   "source": [
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, n_classes=30):\n",
    "        super(QuestModel, self).__init__()\n",
    "        self.model_name = 'QuestModel'\n",
    "        #self.bert_model = BertModel.from_pretrained(BERT_DIR+'/bert-base-uncased/')\n",
    "        #self.bert_model = BertModel.from_pretrained('../input/bert-base-uncased/')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.emb_category = nn.Embedding(len(category_list), 3)\n",
    "        self.emb_host = nn.Embedding(len(host_list), 5)\n",
    "        \n",
    "        self.fc = nn.Linear(768+3+5, n_classes)\n",
    "\n",
    "    def forward(self, ids, seg_ids, category, host):\n",
    "        attention_mask = (ids > 0)  # ids==0([PAD])部分だけFalseとなるので、そこだけattention_weightを0に\n",
    "        layers, pool_out = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n",
    "        #print(layers.size())  # (batch_size,sequence_length, 768)\n",
    "        #print(pool_out.size())  # (batch_size, 768), first token of last layerをいじったもの\n",
    "        \n",
    "        out = F.avg_pool1d(layers.transpose(1,2), kernel_size=layers.size()[1]).squeeze()  # sequence方向は中央値だけ抽出\n",
    "        \n",
    "        emb_category = self.emb_category(category)\n",
    "        emb_host = self.emb_host(host)\n",
    "        \n",
    "        #print(out.shape)\n",
    "        #print(emb_category.shape)\n",
    "        \n",
    "        out = torch.cat([out, emb_category], dim=-1)\n",
    "        out = torch.cat([out, emb_host], dim=-1)\n",
    "        \n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = F.dropout(out, p=0.2, training=self.training)\n",
    "        \n",
    "#         out = F.dropout(layers[-1][:, 0, :], p=0.2, training=self.training)\n",
    "#         out =  F.dropout(pool_out, p=0.2, training=self.training)\n",
    "        logit = self.fc(out)\n",
    "        return logit # 単に30種類の出力値を算出\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffu67MIrOQC4"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, optimizer, criterion, scheduler):\n",
    "    model.train()\n",
    "    avg_loss = 0.    \n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        ids_train, seg_ids_train, category_train, host_train, label_ids_train = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "        \n",
    "        #print(host_train)\n",
    "        \n",
    "        logits = model(ids_train, seg_ids_train, category_train, host_train)\n",
    "        #logits = torch.sigmoid(model(ids_train, seg_ids_train))\n",
    "        \n",
    "        #loss = config.question_weight*criterion(logits[:,0:21], label_ids_train[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], label_ids_train[:,21:30])\n",
    "        loss = criterion(logits, label_ids_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        del ids_train, seg_ids_train, label_ids_train\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return avg_loss\n",
    "\n",
    "def val_model(val_loader, val_length, batch_size=8):\n",
    "    model.eval() # eval mode  \n",
    "    avg_val_loss = 0.\n",
    "    \n",
    "    valid_preds = np.zeros((val_length, len(target_columns)))\n",
    "    original = np.zeros((val_length, len(target_columns)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(val_loader)):\n",
    "            ids_val, seg_ids_val, category_val, host_val, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
    "            \n",
    "            logits = torch.sigmoid(model(ids_val, seg_ids_val, category_val, host_val))\n",
    "            \n",
    "            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n",
    "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
    "            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        score = 0\n",
    "        preds = torch.tensor(valid_preds).numpy()\n",
    "        #preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "        \n",
    "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
    "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
    "        \n",
    "        for i in range(len(target_columns)):\n",
    "            print(i, spearmanr(original[:,i], preds[:,i]))\n",
    "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
    "    \n",
    "    return avg_val_loss, score/len(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8TiUixyOQDE"
   },
   "outputs": [],
   "source": [
    "def calc_spearman(targets, preds):\n",
    "    score = 0\n",
    "    for i in range(targets.shape[1]):\n",
    "        score += np.nan_to_num(spearmanr(targets[:, i], preds[:, i]).correlation)\n",
    "    return score/targets.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAL9M1HxOQDN"
   },
   "outputs": [],
   "source": [
    "ACCUM_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqB6f-cmOQDX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = QuestModel(n_classes=len(target_columns)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, eps=4e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Xz4rSv7VOQDj",
    "outputId": "553f86fe-c3ab-46c7-97e6-6ae43442e846",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---1-Fold---\n",
      "train (4863, 41)\n",
      "val (1216, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 111/607 [01:05<04:50,  1.70it/s]"
     ]
    }
   ],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_preds = 0\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                break\n",
    "    \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIkCm96Vjo_H"
   },
   "outputs": [],
   "source": [
    "def create_model(model_file):\n",
    "    model = QuestModel(n_classes=len(target_columns)).to(device)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model = model\n",
    "    #model = DataParallel(model)\n",
    "    return model\n",
    "\n",
    "def create_models():\n",
    "    models = []\n",
    "    for fold in range(config.fold):\n",
    "        model = create_model('/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict(models, test_loader):\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for ids, seg_ids, category, host in tqdm(test_loader, total=test_loader.num // test_loader.batch_size):\n",
    "            ids, seg_ids, category, host = ids.to(device), seg_ids.to(device), category.to(device), host.to(device)\n",
    "            scores = []\n",
    "            for model in models:\n",
    "                outputs = torch.sigmoid(model(ids, seg_ids, category, host)).cpu()\n",
    "                scores.append(outputs)\n",
    "            all_scores.append(torch.mean(torch.stack(scores), 0))\n",
    "\n",
    "    all_scores = torch.cat(all_scores, 0).numpy()\n",
    "    \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "te6a7J2p-k06"
   },
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(batch_size=32)\n",
    "models = create_models()\n",
    "preds = predict(models, train_loader)\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "cv_score = calc_spearman(train[target_columns].values, preds)\n",
    "print(cv_score)\n",
    "\n",
    "del train_loader, models, preds, train\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}_{}.txt'.format(config.name, cv_score), 'w') as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=4, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='reModel_AI', \\\n",
    "                        question_weight=0.5, \\\n",
    "                        answer_weight=0.5, \\\n",
    "                        fold=5, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_well_written',\n",
    "    'question_type_compare',\n",
    "    'question_type_consequence',\n",
    "    'question_type_definition',\n",
    "    'question_type_entity'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_preds = 0\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                break\n",
    "    \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(batch_size=32)\n",
    "models = create_models()\n",
    "preds = predict(models, train_loader)\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "cv_score = calc_spearman(train[target_columns].values, preds)\n",
    "print(cv_score)\n",
    "\n",
    "del train_loader, models, preds, train\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}_{}.txt'.format(config.name, cv_score), 'w') as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question + Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "#MAX_Q_LEN = 250\n",
    "#MAX_A_LEN = 259\n",
    "SEP_TOKEN_ID = 102 # bert-base-uncasedにおけるvocabの'[SEP']が、102番目という意味\n",
    "\n",
    "class QuestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, train_mode=True, labeled=True):\n",
    "        self.df = df\n",
    "        self.train_mode = train_mode\n",
    "        self.labeled = labeled\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(BERT_DIR+'/bert-base-uncased')\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased/')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        token_id列\n",
    "        segment_id列\n",
    "        label列\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids, seg_ids = self.get_token_ids(row)\n",
    "        if self.labeled:\n",
    "            labels = self.get_label(row)\n",
    "            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host), labels\n",
    "        else:\n",
    "            return token_ids, seg_ids, torch.tensor(row.category), torch.tensor(row.host)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "#     def select_tokens(self, tokens, max_num):\n",
    "#         if len(tokens) <= max_num:\n",
    "#             return tokens\n",
    "#         if self.train_mode:\n",
    "#             num_remove = len(tokens) - max_num\n",
    "#             remove_start = random.randint(0, len(tokens)-num_remove-1)\n",
    "#             return tokens[:remove_start] + tokens[remove_start + num_remove:]\n",
    "#         else:\n",
    "#             return tokens[:max_num//2] + tokens[-(max_num - max_num//2):]\n",
    "\n",
    "    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n",
    "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "        \"\"\"\n",
    "        title. question, answerそれぞれのセンテンスを、tokenizeする\n",
    "        max_lengthに足りない分は、\n",
    "        \"\"\"\n",
    "        t = self.tokenizer.tokenize(title)\n",
    "        q = self.tokenizer.tokenize(question)\n",
    "        a = self.tokenizer.tokenize(answer)\n",
    "\n",
    "        t_len = len(t)\n",
    "        q_len = len(q)\n",
    "        a_len = len(a)\n",
    "\n",
    "        if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "\n",
    "            if t_max_len > t_len:\n",
    "                \"\"\"\n",
    "                titleが短い場合、\n",
    "                最大長に足りない長さを、半分ずつqとaに加える\n",
    "                \"\"\"\n",
    "                t_new_len = t_len\n",
    "                a_max_len = a_max_len + math.floor((t_max_len - t_len)/2) # 切り捨て\n",
    "                q_max_len = q_max_len + math.ceil((t_max_len - t_len)/2) # 切り上げ\n",
    "            else:\n",
    "                \"\"\"\n",
    "                titleが長い場合、最大長で切る\n",
    "                \"\"\"\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "            if a_max_len > a_len:\n",
    "                \"\"\"\n",
    "                answerに加えても短い場合、\n",
    "                最大長に足りない長さを、qに加える\n",
    "                \"\"\"\n",
    "                a_new_len = a_len \n",
    "                q_new_len = q_max_len + (a_max_len - a_len)\n",
    "            elif q_max_len > q_len:\n",
    "                a_new_len = a_max_len + (q_max_len - q_len)\n",
    "                q_new_len = q_len\n",
    "            else:\n",
    "                a_new_len = a_max_len\n",
    "                q_new_len = q_max_len\n",
    "\n",
    "\n",
    "            if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "\n",
    "            t = t[:t_new_len]\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "\n",
    "        return t, q, a\n",
    "        \n",
    "    def get_token_ids(self, row):\n",
    "        t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer)\n",
    "        \n",
    "        # BERTの入力タイプに変換([CLS]と[SEP]をつないで、１つのsetentenceに)\n",
    "        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        if len(token_ids) < MAX_LEN:\n",
    "            \"\"\"0で後ろからpadding\"\"\"\n",
    "            token_ids += [0] * (MAX_LEN - len(token_ids))\n",
    "        ids = torch.tensor(token_ids)\n",
    "        seg_ids = self.get_seg_ids(ids)  # segment_embを区別するindex\n",
    "        return ids, seg_ids\n",
    "    \n",
    "    def get_seg_ids(self, ids):\n",
    "        \"\"\"\n",
    "        いくつめの文かを区別するsegment_idを、各文字に振る\n",
    "        \"\"\"\n",
    "        seg_ids = torch.zeros_like(ids) # [max_len]のtorch_tensor\n",
    "        seg_idx = 0\n",
    "        first_sep = True\n",
    "        for i, e in enumerate(ids):\n",
    "            seg_ids[i] = seg_idx\n",
    "            if e == SEP_TOKEN_ID: # [SEP]の場合\n",
    "                if first_sep:\n",
    "                    first_sep = False\n",
    "                else:\n",
    "                    seg_idx = 1\n",
    "        pad_idx = torch.nonzero(ids == 0)  # bert-base_uncasedのvocabで、[PAD]は0番目であるので、PADの部分のindexだけ抽出\n",
    "        seg_ids[pad_idx] = 0\n",
    "\n",
    "        return seg_ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "        #print(row[target_columns].values)\n",
    "        return torch.tensor(row[target_columns].values.astype(np.float32))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        labelデータを持つモードと、ない完全な推論モードでは、batchのshapeが異なるので(labelが2番目の要素にあるなし)\n",
    "        \"\"\"\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        seg_ids = torch.stack([x[1] for x in batch])\n",
    "        category = torch.stack([x[2] for x in batch])\n",
    "        host = torch.stack([x[3] for x in batch])\n",
    "    \n",
    "        if self.labeled:\n",
    "            labels = torch.stack([x[-1] for x in batch])\n",
    "            return token_ids, seg_ids, category, host, labels\n",
    "        else:\n",
    "            return token_ids, seg_ids, category, host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    df = shuffle(df, random_state=1234)\n",
    "    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        if fold == ifold:\n",
    "            df_train = df.iloc[train_idx]\n",
    "            df_val = df.iloc[valid_idx]\n",
    "            break\n",
    "\n",
    "    print('train', df_train.shape)\n",
    "    print('val', df_val.shape)\n",
    "\n",
    "    ds_train = QuestDataset(df_train, train_mode=True)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n",
    "    train_loader.num = len(df_train)\n",
    "\n",
    "    ds_val = QuestDataset(df_val, train_mode=False)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n",
    "    val_loader.num = len(df_val)\n",
    "    val_loader.df = df_val\n",
    "\n",
    "    return train_loader, val_loader, df_val.shape[0]\n",
    "\n",
    "\n",
    "def get_train_loader(batch_size=4):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=4):\n",
    "    df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "    # cleaning\n",
    "    df['question_title'] = df['question_title'].apply(lambda x : preprocessing(x))\n",
    "    df['question_body'] = df['question_body'].apply(lambda x : preprocessing(x))\n",
    "    df['answer'] = df['answer'].apply(lambda x : preprocessing(x))\n",
    "    \n",
    "    # label encode\n",
    "    le_category = LabelEncoder()\n",
    "    le_category.fit(category_list)\n",
    "    for c in set(df.category):\n",
    "        if c not in category_list:\n",
    "            df.category = df.category.replace(c, np.nan)\n",
    "            df.category = df.category.fillna(train.category.mode()[0])\n",
    "    df.category = le_category.transform(df.category)\n",
    "    \n",
    "    le_host = LabelEncoder()\n",
    "    le_host.fit(host_list)\n",
    "    for c in set(df.host):\n",
    "        if c not in host_list:\n",
    "            df.host = df.host.replace(c, np.nan)\n",
    "            df.host = df.host.fillna(train.host.mode()[0])\n",
    "    df.host = le_host.transform(df.host)\n",
    "\n",
    "\n",
    "    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n",
    "    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n",
    "    loader.num = len(df)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=4, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='reModel_AE', \\\n",
    "                        question_weight=0.5, \\\n",
    "                        answer_weight=0.5, \\\n",
    "                        fold=5, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_well_written',\n",
    "    'question_multi_intent',\n",
    "    'question_type_choice',\n",
    "    'question_type_reason_explanation',\n",
    "    'answer_type_reason_explanation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_preds = 0\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                break\n",
    "    \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(batch_size=32)\n",
    "models = create_models()\n",
    "preds = predict(models, train_loader)\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "cv_score = calc_spearman(train[target_columns].values, preds)\n",
    "print(cv_score)\n",
    "\n",
    "del train_loader, models, preds, train\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}_{}.txt'.format(config.name, cv_score), 'w') as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=4, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='reModel_AF', \\\n",
    "                        question_weight=0.5, \\\n",
    "                        answer_weight=0.5, \\\n",
    "                        fold=5, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_well_written',\n",
    "    'question_type_instructions',\n",
    "    'answer_type_instructions',\n",
    "    'question_type_procedure',\n",
    "    'answer_type_procedure',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_preds = 0\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                break\n",
    "    \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(batch_size=32)\n",
    "models = create_models()\n",
    "preds = predict(models, train_loader)\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "cv_score = calc_spearman(train[target_columns].values, preds)\n",
    "print(cv_score)\n",
    "\n",
    "del train_loader, models, preds, train\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}_{}.txt'.format(config.name, cv_score), 'w') as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipeLineConfig(lr=1e-5, \\\n",
    "                        warmup=0.01, \\\n",
    "                        epochs=4, \\\n",
    "                        patience=3, \\\n",
    "                        batch_size=8, \\\n",
    "                        seed=42, \\\n",
    "                        name='reModel_AG', \\\n",
    "                        question_weight=0.5, \\\n",
    "                        answer_weight=0.5, \\\n",
    "                        fold=5, \\\n",
    "                        train=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_well_written',\n",
    "    'answer_helpful',\n",
    "    'answer_level_of_information',\n",
    "    'answer_plausible',\n",
    "    'answer_relevance',\n",
    "    'answer_satisfaction',\n",
    "    'answer_well_written'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(config.fold):\n",
    "    print('---%d-Fold---'%(fold+1))\n",
    "    \n",
    "    patience = 0\n",
    "    best_loss   = 100.0\n",
    "    best_score      = -1.\n",
    "    best_preds = 0\n",
    "    best_param_loss = None\n",
    "    best_param_score = None\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        start_time   = time.time()\n",
    "        \n",
    "        train_loader, val_loader, val_length = get_train_val_loaders(batch_size=config.batch_size, val_batch_size=config.batch_size, ifold=fold)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= config.epochs*len(train_loader)//ACCUM_STEPS)\n",
    "        \n",
    "        loss_train = train_model(train_loader, optimizer, criterion, scheduler)\n",
    "        loss_val, score_val = val_model(val_loader, val_length, batch_size=config.batch_size)\n",
    "        print(f'Epoch {(epoch+1)}, train_loss: {loss_train}, val_loss: {loss_val}, score_val: {score_val}, time: {(time.time()-start_time)}')\n",
    "        \n",
    "\n",
    "        if score_val > best_score:\n",
    "            best_score = score_val\n",
    "            best_param_score = model.state_dict()\n",
    "            print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "            torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config.patience:\n",
    "                del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                break\n",
    "    \n",
    "        del train_loader, val_loader, loss_train, loss_val, score_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    model.load_state_dict(best_param_score)\n",
    "    print('best_param_score_{}_{}.pt'.format(config.name ,fold+1))\n",
    "    torch.save(best_param_score, '/content/drive/My Drive/Colab Notebooks/GoogleQuest/best_param_score_{}_{}.pt'.format(config.name ,fold+1))   \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(batch_size=32)\n",
    "models = create_models()\n",
    "preds = predict(models, train_loader)\n",
    "train = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "cv_score = calc_spearman(train[target_columns].values, preds)\n",
    "print(cv_score)\n",
    "\n",
    "del train_loader, models, preds, train\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Colab Notebooks/GoogleQuest/{}_{}.txt'.format(config.name, cv_score), 'w') as f:\n",
    "    f.write('')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "16_reModel_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
